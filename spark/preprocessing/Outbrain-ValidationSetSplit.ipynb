{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#OUTPUT_BUCKET_FOLDER = \"gs://<GCS_BUCKET_NAME>/outbrain-click-prediction/output/\"\n",
    "#DATA_BUCKET_FOLDER = \"gs://<GCS_BUCKET_NAME>/outbrain-click-prediction/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "truncate_day_from_timestamp_udf = F.udf(lambda ts: int(ts / 1000 / 60 / 60 / 24), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "events_schema = StructType(\n",
    "                    [StructField(\"display_id\", IntegerType(), True),\n",
    "                    StructField(\"uuid_event\", StringType(), True),                    \n",
    "                    StructField(\"document_id_event\", IntegerType(), True),\n",
    "                    StructField(\"timestamp_event\", IntegerType(), True),\n",
    "                    StructField(\"platform_event\", IntegerType(), True),\n",
    "                    StructField(\"geo_location_event\", StringType(), True)]\n",
    "                    )\n",
    "\n",
    "events_df = spark.read.schema(events_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                .csv(DATA_BUCKET_FOLDER + \"events.csv\") \\\n",
    "                .withColumn('day_event', truncate_day_from_timestamp_udf('timestamp_event')) \\\n",
    "                .alias('events')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "promoted_content_schema = StructType(\n",
    "                    [StructField(\"ad_id\", IntegerType(), True),\n",
    "                    StructField(\"document_id_promo\", IntegerType(), True),                    \n",
    "                    StructField(\"campaign_id\", IntegerType(), True),\n",
    "                    StructField(\"advertiser_id\", IntegerType(), True)]\n",
    "                    )\n",
    "\n",
    "promoted_content_df = spark.read.schema(promoted_content_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                .csv(DATA_BUCKET_FOLDER+\"promoted_content.csv\") \\\n",
    "                .alias('promoted_content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clicks_train_schema = StructType(\n",
    "                    [StructField(\"display_id\", IntegerType(), True),\n",
    "                    StructField(\"ad_id\", IntegerType(), True),                    \n",
    "                    StructField(\"clicked\", IntegerType(), True)]\n",
    "                    )\n",
    "\n",
    "clicks_train_df = spark.read.schema(clicks_train_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                .csv(DATA_BUCKET_FOLDER+\"clicks_train.csv\") \\\n",
    "                .alias('clicks_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clicks_train_joined_df = clicks_train_df \\\n",
    "                         .join(promoted_content_df, on='ad_id', how='left') \\\n",
    "                         .join(events_joined_df, on='display_id', how='left')                         \n",
    "clicks_train_joined_df.createOrReplaceTempView('clicks_train_joined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_display_ids_df = clicks_train_joined_df.select('display_id','day_event').distinct() \\\n",
    "                                .sampleBy(\"day_event\", fractions={0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2, \\\n",
    "                                                                5: 0.2, 6: 0.2, 7: 0.2, 8: 0.2, 9: 0.2, 10: 0.2, \\\n",
    "                                                               11: 1.0, 12: 1.0}, seed=0)   \n",
    "validation_display_ids_df.createOrReplaceTempView(\"validation_display_ids\")                                                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_set_df = spark.sql('''SELECT display_id, ad_id, uuid_event, day_event, timestamp_event,\n",
    "                                        document_id_promo, platform_event, geo_location_event FROM clicks_train_joined t \n",
    "             WHERE EXISTS (SELECT display_id FROM validation_display_ids \n",
    "                           WHERE display_id = t.display_id)''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_set_gcs_output = \"validation_set.parquet\"\n",
    "validation_set_df.write.parquet(OUTPUT_BUCKET_FOLDER+validation_set_gcs_output, mode='overwrite')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
