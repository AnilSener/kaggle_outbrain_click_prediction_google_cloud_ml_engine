{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "evaluation = True\n",
    "evaluation_verbose = False\n",
    "\n",
    "OUTPUT_BUCKET_FOLDER = \"gs://<GCS_BUCKET_NAME>/outbrain-click-prediction/output/\"\n",
    "DATA_BUCKET_FOLDER = \"gs://<GCS_BUCKET_NAME>/outbrain-click-prediction/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.linalg import Vectors, SparseVector, VectorUDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import datetime\n",
    "import time\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "def hashstr(s, nr_bins):\n",
    "    return int(hashlib.md5(s.encode('utf8')).hexdigest(), 16)%(nr_bins-1)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def date_time_to_unix_epoch(date_time):\n",
    "    return int(time.mktime(date_time.timetuple()))\n",
    "\n",
    "def date_time_to_unix_epoch_treated(dt):\n",
    "    if dt != None:\n",
    "        try:\n",
    "            epoch = date_time_to_unix_epoch(dt)\n",
    "            return epoch\n",
    "        except Exception as e:\n",
    "            print(\"Error processing dt={}\".format(dt), e)\n",
    "            return 0\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "timestamp_null_to_zero_int_udf = F.udf(lambda x: date_time_to_unix_epoch_treated(x), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "INT_DEFAULT_NULL_VALUE = -1\n",
    "int_null_to_minus_one_udf = F.udf(lambda x: x if x != None else INT_DEFAULT_NULL_VALUE, IntegerType())\n",
    "int_list_null_to_empty_list_udf = F.udf(lambda x: x if x != None else [], ArrayType(IntegerType()))\n",
    "float_list_null_to_empty_list_udf = F.udf(lambda x: x if x != None else [], ArrayType(FloatType()))\n",
    "str_list_null_to_empty_list_udf = F.udf(lambda x: x if x != None else [], ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def truncate_day_from_timestamp(ts):\n",
    "    return int(ts / 1000 / 60 / 60 / 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "truncate_day_from_timestamp_udf = F.udf(lambda ts: truncate_day_from_timestamp(ts), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "extract_country_udf = F.udf(lambda geo: geo.strip()[:2] if geo != None else '', StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "extract_country_state_udf = F.udf(lambda geo: geo.strip()[:5] if geo != None else '', StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "list_len_udf = F.udf(lambda x: len(x) if x != None else 0, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def convert_odd_timestamp(timestamp_ms_relative):\n",
    "    TIMESTAMP_DELTA=1465876799998\n",
    "    return datetime.datetime.fromtimestamp((int(timestamp_ms_relative)+TIMESTAMP_DELTA)//1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Loading Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Loading UTC/BST for each country and US / CA states (local time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "country_utc_dst_df = pd.read_csv('aux_data/country_codes_utc_dst_tz_delta.csv', keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "countries_utc_dst_dict = dict(zip(country_utc_dst_df['country_code'].tolist(), country_utc_dst_df['utc_dst_time_offset_cleaned'].tolist()))\n",
    "countries_utc_dst_broad = sc.broadcast(countries_utc_dst_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "us_states_utc_dst_df = pd.read_csv('aux_data/us_states_abbrev_bst.csv', keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "us_states_utc_dst_dict = dict(zip(us_states_utc_dst_df['state_abb'].tolist(), us_states_utc_dst_df['utc_dst_time_offset_cleaned'].tolist()))\n",
    "us_states_utc_dst_broad = sc.broadcast(us_states_utc_dst_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ca_states_utc_dst_df = pd.read_csv('aux_data/ca_states_abbrev_bst.csv', keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ca_countries_utc_dst_dict = dict(zip(ca_states_utc_dst_df['state_abb'].tolist(), ca_states_utc_dst_df['utc_dst_time_offset_cleaned'].tolist()))\n",
    "ca_countries_utc_dst_broad = sc.broadcast(ca_countries_utc_dst_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Loading competition csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "events_schema = StructType(\n",
    "                    [StructField(\"display_id\", IntegerType(), True),\n",
    "                    StructField(\"uuid_event\", StringType(), True),                    \n",
    "                    StructField(\"document_id_event\", IntegerType(), True),\n",
    "                    StructField(\"timestamp_event\", IntegerType(), True),\n",
    "                    StructField(\"platform_event\", IntegerType(), True),\n",
    "                    StructField(\"geo_location_event\", StringType(), True)]\n",
    "                    )\n",
    "\n",
    "events_df = spark.read.schema(events_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                .csv(DATA_BUCKET_FOLDER + \"events.csv\") \\\n",
    "                .withColumn('dummyEvents', F.lit(1)) \\\n",
    "                .withColumn('day_event', truncate_day_from_timestamp_udf('timestamp_event')) \\\n",
    "                .withColumn('event_country', extract_country_udf('geo_location_event')) \\\n",
    "                .withColumn('event_country_state', extract_country_state_udf('geo_location_event')) \\\n",
    "                .alias('events')               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "page_views_schema = StructType(\n",
    "                    [StructField(\"uuid_pv\", StringType(), True),\n",
    "                    StructField(\"document_id_pv\", IntegerType(), True),\n",
    "                    StructField(\"timestamp_pv\", IntegerType(), True),\n",
    "                    StructField(\"platform_pv\", IntegerType(), True),\n",
    "                    StructField(\"geo_location_pv\", StringType(), True),\n",
    "                    StructField(\"traffic_source_pv\", IntegerType(), True)]\n",
    "                    )\n",
    "page_views_df = spark.read.schema(page_views_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                .csv(DATA_BUCKET_FOLDER+\"page_views.csv\") \\\n",
    "                .withColumn('day_pv', truncate_day_from_timestamp_udf('timestamp_pv')) \\\n",
    "                .alias('page_views')        \n",
    "            \n",
    "page_views_df.createOrReplaceTempView('page_views')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "page_views_users_df  = spark.sql('''\n",
    "                    SELECT uuid_pv, document_id_pv, max(timestamp_pv) as max_timestamp_pv, 1 as dummyPageView\n",
    "                    FROM page_views p \n",
    "                    GROUP BY uuid_pv, document_id_pv\n",
    "                    ''').alias('page_views_users')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "promoted_content_schema = StructType(\n",
    "                    [StructField(\"ad_id\", IntegerType(), True),\n",
    "                    StructField(\"document_id_promo\", IntegerType(), True),                    \n",
    "                    StructField(\"campaign_id\", IntegerType(), True),\n",
    "                    StructField(\"advertiser_id\", IntegerType(), True)]\n",
    "                    )\n",
    "\n",
    "promoted_content_df = spark.read.schema(promoted_content_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                .csv(DATA_BUCKET_FOLDER+\"promoted_content.csv\") \\\n",
    "                .withColumn('dummyPromotedContent', F.lit(1)).alias('promoted_content').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "documents_meta_schema = StructType(\n",
    "                    [StructField(\"document_id_doc\", IntegerType(), True),\n",
    "                    StructField(\"source_id\", IntegerType(), True),                    \n",
    "                    StructField(\"publisher_id\", IntegerType(), True),\n",
    "                    StructField(\"publish_time\", TimestampType(), True)]\n",
    "                    )\n",
    "\n",
    "documents_meta_df = spark.read.schema(documents_meta_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                .csv(DATA_BUCKET_FOLDER+\"documents_meta.csv\") \\\n",
    "                .withColumn('dummyDocumentsMeta', F.lit(1)).alias('documents_meta').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Joining with Page Views to get traffic_source_pv\n",
    "events_joined_df = events_df.join(documents_meta_df \\\n",
    "                                  .withColumnRenamed('source_id', 'source_id_doc_event') \\\n",
    "                                  .withColumnRenamed('publisher_id', 'publisher_doc_event') \\\n",
    "                                  .withColumnRenamed('publish_time', 'publish_time_doc_event')\n",
    "                                  , on=F.col(\"document_id_event\") == F.col(\"document_id_doc\"), how='left') \\\n",
    "                            .join(page_views_df, \n",
    "                                           on=[F.col('uuid_event') == F.col('uuid_pv'),\n",
    "                                               F.col('document_id_event') == F.col('document_id_pv'),\n",
    "                                               F.col('platform_event') == F.col('platform_pv'),\n",
    "                                               F.col('geo_location_event') == F.col('geo_location_pv'),\n",
    "                                               F.col('day_event') == F.col('day_pv')],\n",
    "                                           how='left') \\\n",
    "                                    .alias('events').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "documents_categories_schema = StructType(\n",
    "                    [StructField(\"document_id_cat\", IntegerType(), True),\n",
    "                    StructField(\"category_id\", IntegerType(), True),                    \n",
    "                    StructField(\"confidence_level_cat\", FloatType(), True)]\n",
    "                    )\n",
    "\n",
    "documents_categories_df = spark.read.schema(documents_categories_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                .csv(DATA_BUCKET_FOLDER+\"documents_categories.csv\") \\\n",
    "                .alias('documents_categories').cache()\n",
    "    \n",
    "documents_categories_grouped_df = documents_categories_df.groupBy('document_id_cat') \\\n",
    "                                            .agg(F.collect_list('category_id').alias('category_id_list'),\n",
    "                                                 F.collect_list('confidence_level_cat').alias('confidence_level_cat_list')) \\\n",
    "                                            .withColumn('dummyDocumentsCategory', F.lit(1)) \\\n",
    "                                            .alias('documents_categories_grouped')                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "documents_topics_schema = StructType(\n",
    "                    [StructField(\"document_id_top\", IntegerType(), True),\n",
    "                    StructField(\"topic_id\", IntegerType(), True),                    \n",
    "                    StructField(\"confidence_level_top\", FloatType(), True)]\n",
    "                    )\n",
    "\n",
    "documents_topics_df = spark.read.schema(documents_topics_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                .csv(DATA_BUCKET_FOLDER+\"documents_topics.csv\")  \\\n",
    "                .alias('documents_topics').cache()\n",
    "    \n",
    "documents_topics_grouped_df = documents_topics_df.groupBy('document_id_top') \\\n",
    "                                            .agg(F.collect_list('topic_id').alias('topic_id_list'),\n",
    "                                                 F.collect_list('confidence_level_top').alias('confidence_level_top_list')) \\\n",
    "                                            .withColumn('dummyDocumentsTopics', F.lit(1)) \\\n",
    "                                            .alias('documents_topics_grouped')                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "documents_entities_schema = StructType(\n",
    "                    [StructField(\"document_id_ent\", IntegerType(), True),\n",
    "                    StructField(\"entity_id\", StringType(), True),                    \n",
    "                    StructField(\"confidence_level_ent\", FloatType(), True)]\n",
    "                    )\n",
    "\n",
    "documents_entities_df = spark.read.schema(documents_entities_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                .csv(DATA_BUCKET_FOLDER+\"documents_entities.csv\")  \\\n",
    "                .alias('documents_entities').cache()\n",
    "    \n",
    "documents_entities_grouped_df = documents_entities_df.groupBy('document_id_ent') \\\n",
    "                                            .agg(F.collect_list('entity_id').alias('entity_id_list'),\n",
    "                                                 F.collect_list('confidence_level_ent').alias('confidence_level_ent_list')) \\\n",
    "                                            .withColumn('dummyDocumentsEntities', F.lit(1)) \\\n",
    "                                            .alias('documents_entities_grouped')                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "clicks_train_schema = StructType(\n",
    "                    [StructField(\"display_id\", IntegerType(), True),\n",
    "                    StructField(\"ad_id\", IntegerType(), True),                    \n",
    "                    StructField(\"clicked\", IntegerType(), True)]\n",
    "                    )\n",
    "\n",
    "clicks_train_df = spark.read.schema(clicks_train_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                .csv(DATA_BUCKET_FOLDER+\"clicks_train.csv\") \\\n",
    "                .withColumn('dummyClicksTrain', F.lit(1)).alias('clicks_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "clicks_train_joined_df = clicks_train_df \\\n",
    "                         .join(promoted_content_df, on='ad_id', how='left') \\\n",
    "                         .join(documents_meta_df, on=F.col(\"promoted_content.document_id_promo\") == F.col(\"documents_meta.document_id_doc\"), how='left') \\\n",
    "                         .join(events_joined_df, on='display_id', how='left')                         \n",
    "clicks_train_joined_df.createOrReplaceTempView('clicks_train_joined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if evaluation:\n",
    "    table_name = 'user_profiles_eval'\n",
    "else:\n",
    "    table_name = 'user_profiles' \n",
    "\n",
    "user_profiles_df = spark.read.parquet(OUTPUT_BUCKET_FOLDER+table_name) \\\n",
    "                    .withColumn('dummyUserProfiles', F.lit(1)).alias('user_profiles')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Spliting Train/validation set | Test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For a fixed validation set, below cell should be executed only once, and in the next notebook executions it would take the saved DataFrame temp view (validation_display_ids) to do the same split train/validation split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if evaluation:    \n",
    "    \n",
    "    validation_display_ids_df = clicks_train_joined_df.select('display_id','day_event').distinct() \\\n",
    "                                                    .sampleBy(\"day_event\", fractions={0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2, \\\n",
    "                                                                                5: 0.2, 6: 0.2, 7: 0.2, 8: 0.2, 9: 0.2, 10: 0.2, \\\n",
    "                                                                               11: 1.0, 12: 1.0}, seed=0) \\\n",
    "                                                    .cache()    \n",
    "    validation_display_ids_df.createOrReplaceTempView(\"validation_display_ids\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train_set_df.count() =', 59761474)\n"
     ]
    }
   ],
   "source": [
    "if evaluation:    \n",
    "    validation_set_exported_schema = StructType(\n",
    "                        [StructField(\"display_id\", IntegerType(), True),\n",
    "                        StructField(\"ad_id\", IntegerType(), True), \n",
    "                        StructField(\"uuid_event\", StringType(), True),\n",
    "                        StructField(\"document_id_promo\", IntegerType(), True),\n",
    "                        StructField(\"platform_event\", IntegerType(), True),\n",
    "                        StructField(\"geo_location_event\", StringType(), True)]\n",
    "                        )\n",
    "    \n",
    "    validation_set_exported_df = spark.read.schema(validation_set_exported_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                    .csv(OUTPUT_BUCKET_FOLDER + \"pickles/validation_set.csv\")\n",
    "            \n",
    "    validation_set_exported_df.select('display_id').distinct().createOrReplaceTempView(\"validation_display_ids\")\n",
    "    \n",
    "    \n",
    "    validation_set_df = spark.sql('''SELECT * FROM clicks_train_joined t \n",
    "             WHERE EXISTS (SELECT display_id FROM validation_display_ids \n",
    "                           WHERE display_id = t.display_id)''').alias('clicks') \\\n",
    "                         .join(documents_categories_grouped_df, on=F.col(\"document_id_promo\") == F.col(\"documents_categories_grouped.document_id_cat\"), how='left') \\\n",
    "                         .join(documents_topics_grouped_df, on=F.col(\"document_id_promo\") == F.col(\"documents_topics_grouped.document_id_top\"), how='left') \\\n",
    "                         .join(documents_entities_grouped_df, on=F.col(\"document_id_promo\") == F.col(\"documents_entities_grouped.document_id_ent\"), how='left') \\\n",
    "                         .join(documents_categories_grouped_df \\\n",
    "                                   .withColumnRenamed('category_id_list', 'doc_event_category_id_list')\n",
    "                                   .withColumnRenamed('confidence_level_cat_list', 'doc_event_confidence_level_cat_list') \\\n",
    "                                   .alias('documents_event_categories_grouped'), \n",
    "                               on=F.col(\"document_id_event\") == F.col(\"documents_event_categories_grouped.document_id_cat\"), \n",
    "                               how='left') \\\n",
    "                         .join(documents_topics_grouped_df \\\n",
    "                                   .withColumnRenamed('topic_id_list', 'doc_event_topic_id_list')\n",
    "                                   .withColumnRenamed('confidence_level_top_list', 'doc_event_confidence_level_top_list') \\\n",
    "                                   .alias('documents_event_topics_grouped'), \n",
    "                               on=F.col(\"document_id_event\") == F.col(\"documents_event_topics_grouped.document_id_top\"), \n",
    "                               how='left') \\\n",
    "                         .join(documents_entities_grouped_df \\\n",
    "                                   .withColumnRenamed('entity_id_list', 'doc_event_entity_id_list')\n",
    "                                   .withColumnRenamed('confidence_level_ent_list', 'doc_event_confidence_level_ent_list') \\\n",
    "                                   .alias('documents_event_entities_grouped'), \n",
    "                               on=F.col(\"document_id_event\") == F.col(\"documents_event_entities_grouped.document_id_ent\"), \n",
    "                               how='left') \\\n",
    "                         .join(page_views_users_df, on=[F.col(\"clicks.uuid_event\") == F.col(\"page_views_users.uuid_pv\"),\n",
    "                                                        F.col(\"clicks.document_id_promo\") == F.col(\"page_views_users.document_id_pv\")], \n",
    "                                                  how='left')\n",
    "    \n",
    "    #print(\"validation_set_df.count() =\", validation_set_df.count())\n",
    "        \n",
    "    #Added to validation set information about the event and the user for statistics of the error (avg ctr)\n",
    "    validation_set_ground_truth_df = validation_set_df.filter('clicked = 1') \\\n",
    "                                .join(user_profiles_df, on=[F.col(\"user_profiles.uuid\") == F.col(\"uuid_event\")], how='left') \\\n",
    "                                .withColumn('user_categories_count', list_len_udf('category_id_list')) \\\n",
    "                                .withColumn('user_topics_count', list_len_udf('topic_id_list')) \\\n",
    "                                .withColumn('user_entities_count', list_len_udf('entity_id_list')) \\\n",
    "                                .select('display_id','ad_id','platform_event', 'day_event', 'timestamp_event', \n",
    "                                        'geo_location_event', 'event_country', 'event_country_state', 'views',\n",
    "                                        'user_categories_count', 'user_topics_count', 'user_entities_count') \\\n",
    "                                .withColumnRenamed('ad_id','ad_id_gt') \\\n",
    "                                .withColumnRenamed('views','user_views_count') \\\n",
    "                                .cache()\n",
    "    #print(\"validation_set_ground_truth_df.count() =\", validation_set_ground_truth_df.count())\n",
    "    \n",
    "    train_set_df = spark.sql('''SELECT * FROM clicks_train_joined t \n",
    "                                 WHERE NOT EXISTS (SELECT display_id FROM validation_display_ids \n",
    "                                               WHERE display_id = t.display_id)''').cache()\n",
    "    print(\"train_set_df.count() =\", train_set_df.count())\n",
    "    \n",
    "    #validation_display_ids_df.groupBy(\"day_event\").count().show()\n",
    "    \n",
    "else:\n",
    "    \n",
    "    clicks_test_schema = StructType(\n",
    "                    [StructField(\"display_id\", IntegerType(), True),\n",
    "                    StructField(\"ad_id\", IntegerType(), True)]\n",
    "                    )\n",
    "\n",
    "    clicks_test_df = spark.read.schema(clicks_test_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                    .csv(DATA_BUCKET_FOLDER + \"clicks_test.csv\") \\\n",
    "                    .withColumn('dummyClicksTest', F.lit(1)) \\\n",
    "                    .withColumn('clicked', F.lit(-999)) \\\n",
    "                    .alias('clicks_test')\n",
    "        \n",
    "        \n",
    "    test_set_df = clicks_test_df \\\n",
    "                         .join(promoted_content_df, on='ad_id', how='left') \\\n",
    "                         .join(documents_meta_df, on=F.col(\"promoted_content.document_id_promo\") == F.col(\"documents_meta.document_id_doc\"), how='left') \\\n",
    "                         .join(documents_categories_grouped_df, on=F.col(\"document_id_promo\") == F.col(\"documents_categories_grouped.document_id_cat\"), how='left') \\\n",
    "                         .join(documents_topics_grouped_df, on=F.col(\"document_id_promo\") == F.col(\"documents_topics_grouped.document_id_top\"), how='left') \\\n",
    "                         .join(documents_entities_grouped_df, on=F.col(\"document_id_promo\") == F.col(\"documents_entities_grouped.document_id_ent\"), how='left') \\\n",
    "                         .join(events_joined_df, on='display_id', how='left') \\\n",
    "                         .join(documents_categories_grouped_df \\\n",
    "                                   .withColumnRenamed('category_id_list', 'doc_event_category_id_list')\n",
    "                                   .withColumnRenamed('confidence_level_cat_list', 'doc_event_confidence_level_cat_list') \\\n",
    "                                   .alias('documents_event_categories_grouped'), \n",
    "                               on=F.col(\"document_id_event\") == F.col(\"documents_event_categories_grouped.document_id_cat\"), \n",
    "                               how='left') \\\n",
    "                         .join(documents_topics_grouped_df \\\n",
    "                                   .withColumnRenamed('topic_id_list', 'doc_event_topic_id_list')\n",
    "                                   .withColumnRenamed('confidence_level_top_list', 'doc_event_confidence_level_top_list') \\\n",
    "                                   .alias('documents_event_topics_grouped'), \n",
    "                               on=F.col(\"document_id_event\") == F.col(\"documents_event_topics_grouped.document_id_top\"), \n",
    "                               how='left') \\\n",
    "                         .join(documents_entities_grouped_df \\\n",
    "                                   .withColumnRenamed('entity_id_list', 'doc_event_entity_id_list')\n",
    "                                   .withColumnRenamed('confidence_level_ent_list', 'doc_event_confidence_level_ent_list') \\\n",
    "                                   .alias('documents_event_entities_grouped'), \n",
    "                               on=F.col(\"document_id_event\") == F.col(\"documents_event_entities_grouped.document_id_ent\"), \n",
    "                               how='left') \\\n",
    "                         .join(page_views_users_df, on=[F.col(\"events.uuid_event\") == F.col(\"page_views_users.uuid_pv\"),\n",
    "                                                        F.col(\"promoted_content.document_id_promo\") == F.col(\"page_views_users.document_id_pv\")], \n",
    "                                                  how='left')\n",
    "\n",
    "    #print(\"test_set_df.count() =\",test_set_df.count())\n",
    "   \n",
    "    \n",
    "    train_set_df = clicks_train_joined_df.cache() \n",
    "    print(\"train_set_df.count() =\", train_set_df.count())       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def is_null(value):\n",
    "    return value == None or len(str(value).strip()) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "LESS_SPECIAL_CAT_VALUE = 'less'\n",
    "def get_category_field_values_counts(field, df, min_threshold=10):\n",
    "    category_counts = dict(filter(lambda x: not is_null(x[0]) and x[1] >= min_threshold, df.select(field).groupBy(field).count().rdd.map(lambda x: (x[0], x[1])).collect()))\n",
    "    #Adding a special value to create a feature for values in this category that are less than min_threshold \n",
    "    category_counts[LESS_SPECIAL_CAT_VALUE] = -1\n",
    "    return category_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Building category values counters and indexers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_country_values_counts = get_category_field_values_counts('event_country', events_df, min_threshold=10)\n",
    "event_country_values = sorted(event_country_values_counts.keys())\n",
    "len(event_country_values)\n",
    "#All non-null categories: 230"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1892"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_country_state_values_counts = get_category_field_values_counts('event_country_state', events_df, min_threshold=10)\n",
    "event_country_state_values = sorted(event_country_state_values_counts.keys())\n",
    "len(event_country_state_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2273"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_geo_location_values_counts = get_category_field_values_counts('geo_location_event', events_df, min_threshold=10)\n",
    "event_geo_location_values = sorted(event_geo_location_values_counts.keys())\n",
    "len(event_geo_location_values)\n",
    "#All non-null categories: 2988"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2052"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ad_advertiser_id_values_counts = get_category_field_values_counts('advertiser_id', promoted_content_df, min_threshold=10)\n",
    "ad_advertiser_id_values = sorted(ad_advertiser_id_values_counts.keys())\n",
    "len(ad_advertiser_id_values)\n",
    "#All non-null categories: 4385"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6339"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_source_id_values_counts = get_category_field_values_counts('source_id', documents_meta_df, min_threshold=10)\n",
    "doc_source_id_values  = sorted(doc_source_id_values_counts.keys())\n",
    "len(doc_source_id_values)\n",
    "#All non-null categories: 14394"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "830"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_publisher_id_values_count = get_category_field_values_counts('publisher_id', documents_meta_df, min_threshold=10)\n",
    "doc_publisher_id_values = sorted(doc_publisher_id_values_count.keys())\n",
    "len(doc_publisher_id_values)\n",
    "#All non-null categories: 1259"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_category_id_values_counts = get_category_field_values_counts('category_id', documents_categories_df, min_threshold=10)\n",
    "doc_category_id_values = sorted(doc_category_id_values_counts.keys())\n",
    "len(doc_category_id_values)\n",
    "#All non-null categories: 97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "301"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topic_id_values_counts =  get_category_field_values_counts('topic_id', documents_topics_df, min_threshold=10)\n",
    "doc_topic_id_values = sorted(doc_topic_id_values_counts.keys())\n",
    "len(doc_topic_id_values)\n",
    "#All non-null categories: 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52439"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_entity_id_values_counts = get_category_field_values_counts('entity_id', documents_entities_df, min_threshold=10)\n",
    "doc_entity_id_values = sorted(doc_entity_id_values_counts.keys())\n",
    "len(doc_entity_id_values)\n",
    "#All non-null categories: 1326009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "event_platform_values_counts = get_category_field_values_counts('platform_event', events_df, min_threshold=10)\n",
    "event_platform_values = sorted(event_platform_values_counts.keys())\n",
    "'''\n",
    "event_platform_values = [1, 2, 3]\n",
    "len(event_platform_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traffic_source_pv_values = [1, 2, 3]\n",
    "len(traffic_source_pv_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Sorted by ad_id model (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sort_by_ad_id_udf =  F.udf(lambda ad_ids: sorted(ad_ids), ArrayType(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if evaluation:\n",
    "    data_df = validation_set_df\n",
    "else:\n",
    "    data_df = test_set_df\n",
    "\n",
    "validation_set_predicted_df = data_df.select('display_id','ad_id').groupby('display_id') \\\n",
    "                                .agg(F.collect_set('ad_id').alias('ad_ids_set')) \\\n",
    "                                .withColumn('ad_ids', sort_by_ad_id_udf('ad_ids_set')).select('display_id', 'ad_ids')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Random baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "random_sort_udf =  F.udf(lambda ad_ids: random.sample(ad_ids, len(ad_ids)), ArrayType(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if evaluation:\n",
    "    data_df = validation_set_df\n",
    "else:\n",
    "    data_df = test_set_df\n",
    "\n",
    "validation_set_predicted_df = data_df.select('display_id','ad_id').groupby('display_id') \\\n",
    "                                .agg(F.collect_set('ad_id').alias('ad_ids_set')) \\\n",
    "                                .withColumn('ad_ids', random_sort_udf('ad_ids_set')).select('display_id', 'ad_ids')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Processing average CTR by categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_percentiles(df, field, quantiles_levels=None, max_error_rate=0.0):\n",
    "    if quantiles_levels == None:\n",
    "        quantiles_levels = np.arange(0.0, 1.1, 0.1).tolist() \n",
    "    quantiles = df.approxQuantile(field, quantiles_levels, max_error_rate)\n",
    "    return dict(zip(quantiles_levels, quantiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#REG = 10\n",
    "REG = 0\n",
    "ctr_udf = F.udf(lambda clicks, views: clicks / float(views + REG), FloatType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Average CTR by ad_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ad_id_popularity_df = train_set_df.groupby('ad_id').agg(F.sum('clicked').alias('clicks'), \n",
    "                                                               F.count('*').alias('views')) \\\n",
    "                                         .withColumn('ctr', ctr_udf('clicks','views'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#ad_id_popularity_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#get_percentiles(ad_id_popularity_df, 'clicks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#get_percentiles(ad_id_popularity_df, 'views')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ad_id_popularity = ad_id_popularity_df.filter('views > 5').select('ad_id', 'ctr', 'views') \\\n",
    "                    .rdd.map(lambda x: (x['ad_id'], (x['ctr'], x['views'], 1, 1))).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ad_id_popularity_broad = sc.broadcast(ad_id_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.07692307978868484, 13, 1, 1),\n",
       " (0.08661417663097382, 127, 1, 1),\n",
       " (0.11999999731779099, 25, 1, 1)]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ad_id_popularity.values())[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192107"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ad_id_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#get_ad_id_ctr_udf = F.udf(lambda ad_id: ad_id_popularity[ad_id] if ad_id in ad_id_popularity else -1, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1552830593979102"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ad_id_avg_ctr = sum(map(lambda x: x[0], ad_id_popularity.values())) / float(len(ad_id_popularity))\n",
    "ad_id_avg_ctr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19405337738275"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ad_id_weighted_avg_ctr = sum(map(lambda x: x[0]*x[1], ad_id_popularity.values())) / float(sum(map(lambda x: x[1], ad_id_popularity.values())))\n",
    "ad_id_weighted_avg_ctr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.0"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ad_id_views_median = np.median(np.array(map(lambda x: x[1], ad_id_popularity.values())))\n",
    "ad_id_views_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "308.59291436543174"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ad_id_views_mean = sum(map(lambda x: x[1], ad_id_popularity.values())) / float(len(ad_id_popularity))\n",
    "ad_id_views_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Average CTR by document_id (promoted_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74766"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_id_popularity_df = train_set_df.groupby('document_id_promo').agg(F.sum('clicked').alias('clicks'), \n",
    "                                                               F.count('*').alias('views'),\n",
    "                                                               F.countDistinct('ad_id').alias('distinct_ad_ids')) \\\n",
    "                                         .withColumn('ctr', ctr_udf('clicks','views'))\n",
    "    \n",
    "document_id_popularity = document_id_popularity_df.filter('views > 5').select('document_id_promo', 'ctr', 'views', 'distinct_ad_ids') \\\n",
    "                                                .rdd.map(lambda x: (x['document_id_promo'], (x['ctr'], x['views'], x['distinct_ad_ids'], 1))).collectAsMap()\n",
    "len(document_id_popularity)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "document_id_popularity_broad = sc.broadcast(document_id_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#document_id_popularity_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#get_percentiles(document_id_popularity_df, 'clicks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#get_percentiles(document_id_popularity_df, 'views')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15048812200823822"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_id_avg_ctr = sum(map(lambda x: x[0], document_id_popularity.values())) / float(len(document_id_popularity))\n",
    "document_id_avg_ctr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19380676920446974"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_id_weighted_avg_ctr = sum(map(lambda x: x[0]*x[1], document_id_popularity.values())) / float(sum(map(lambda x: x[1], document_id_popularity.values())))\n",
    "document_id_weighted_avg_ctr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.0"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_id_views_median = np.median(np.array(map(lambda x: x[1], document_id_popularity.values())))\n",
    "document_id_views_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "797.3970253858706"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_id_views_mean = sum(map(lambda x: x[1], document_id_popularity.values())) / float(len(document_id_popularity))\n",
    "document_id_views_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Average CTR by (doc_event, doc_ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1302421"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_event_doc_ad_avg_ctr_df = train_set_df.groupBy('document_id_event', 'document_id_promo') \\\n",
    "                                    .agg(F.sum('clicked').alias('clicks'), \n",
    "                                         F.count('*').alias('views'),\n",
    "                                         F.countDistinct('ad_id').alias('distinct_ad_ids')) \\\n",
    "                                    .withColumn('ctr', ctr_udf('clicks','views'))\n",
    "        \n",
    "doc_event_doc_ad_avg_ctr = doc_event_doc_ad_avg_ctr_df.filter('views > 5') \\\n",
    "                    .select('document_id_event', 'document_id_promo','ctr', 'views', 'distinct_ad_ids') \\\n",
    "                    .rdd.map(lambda x: ((x['document_id_event'], x['document_id_promo']), (x['ctr'], x['views'], x['distinct_ad_ids'], 1))).collectAsMap()        \n",
    "\n",
    "len(doc_event_doc_ad_avg_ctr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "doc_event_doc_ad_avg_ctr_broad = sc.broadcast(doc_event_doc_ad_avg_ctr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Average CTR by country, source_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29856"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_id_by_country_popularity_df = train_set_df.select('clicked', 'source_id', 'event_country', 'ad_id') \\\n",
    "                                            .groupby('event_country', 'source_id').agg(F.sum('clicked').alias('clicks'), \n",
    "                                                                             F.count('*').alias('views'),\n",
    "                                                                             F.countDistinct('ad_id').alias('distinct_ad_ids')) \\\n",
    "                                         .withColumn('ctr', ctr_udf('clicks','views'))\n",
    "        \n",
    "#source_id_popularity = source_id_popularity_df.filter('views > 100 and source_id is not null').select('source_id', 'ctr').rdd.collectAsMap()\n",
    "source_id_by_country_popularity = source_id_by_country_popularity_df.filter('views > 5 and source_id is not null and event_country <> \"\"').select('event_country', 'source_id', 'ctr', 'views', 'distinct_ad_ids') \\\n",
    "        .rdd.map(lambda x: ((x['event_country'], x['source_id']), (x['ctr'], x['views'], x['distinct_ad_ids'], 1))).collectAsMap()\n",
    "len(source_id_by_country_popularity)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "source_id_by_country_popularity_broad = sc.broadcast(source_id_by_country_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18602978924640837"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_id_by_country_avg_ctr = sum(map(lambda x: x[0], source_id_by_country_popularity.values())) / float(len(source_id_by_country_popularity))\n",
    "source_id_by_country_avg_ctr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19364919749745055"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_id_by_country_weighted_avg_ctr = sum(map(lambda x: x[0]*x[1], source_id_by_country_popularity.values())) / float(sum(map(lambda x: x[1], source_id_by_country_popularity.values())))\n",
    "source_id_by_country_weighted_avg_ctr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38.0"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_id_by_country_views_median = np.median(np.array(map(lambda x: x[1], source_id_by_country_popularity.values())))\n",
    "source_id_by_country_views_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1999.1523311897106"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_id_by_country_views_mean = sum(map(lambda x: x[1], source_id_by_country_popularity.values())) / float(len(source_id_by_country_popularity))\n",
    "source_id_by_country_views_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Average CTR by source_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5628"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_id_popularity_df = train_set_df.select('clicked', 'source_id', 'ad_id') \\\n",
    "                                            .groupby('source_id').agg(F.sum('clicked').alias('clicks'), \n",
    "                                                                     F.count('*').alias('views'),\n",
    "                                                                     F.countDistinct('ad_id').alias('distinct_ad_ids')) \\\n",
    "                                         .withColumn('ctr', ctr_udf('clicks','views'))\n",
    "        \n",
    "source_id_popularity = source_id_popularity_df.filter('views > 10 and source_id is not null').select('source_id', 'ctr', 'views', 'distinct_ad_ids') \\\n",
    "                            .rdd.map(lambda x: (x['source_id'], (x['ctr'], x['views'], x['distinct_ad_ids'], 1))).collectAsMap()\n",
    "len(source_id_popularity)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "source_id_popularity_broad = sc.broadcast(source_id_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#source_id_popularity_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#get_percentiles(source_id_popularity_df, 'clicks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#get_percentiles(source_id_popularity_df, 'views')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#source_id_popularity = source_id_popularity_df.filter('views > 100 and source_id is not null').select('source_id', 'ctr').rdd.collectAsMap()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Average CTR by publisher_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "723"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "publisher_popularity_df = train_set_df.select('clicked', 'publisher_id', 'ad_id') \\\n",
    "                                            .groupby('publisher_id').agg(F.sum('clicked').alias('clicks'), \n",
    "                                                                             F.count('*').alias('views'),\n",
    "                                                                              F.countDistinct('ad_id').alias('distinct_ad_ids')) \\\n",
    "                                         .withColumn('ctr', ctr_udf('clicks','views'))\n",
    "        \n",
    "publisher_popularity = publisher_popularity_df.filter('views > 10 and publisher_id is not null').select('publisher_id', 'ctr', 'views', 'distinct_ad_ids') \\\n",
    "                            .rdd.map(lambda x: (x['publisher_id'], (x['ctr'], x['views'], x['distinct_ad_ids'], 1))).collectAsMap()\n",
    "len(publisher_popularity)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "publisher_popularity_broad = sc.broadcast(publisher_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#publisher_popularity_df.count()\n",
    "##863"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#get_percentiles(publisher_popularity_df, 'clicks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#get_percentiles(publisher_popularity_df, 'views')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#publisher_id_popularity = publisher_popularity_df.filter('views > 100 and publisher_id is not null').select('publisher_id', 'ctr').rdd.collectAsMap()\n",
    "#len(publisher_id_popularity)\n",
    "##639"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Average CTR by advertiser_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3620"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "advertiser_id_popularity_df = train_set_df.select('clicked', 'advertiser_id', 'ad_id') \\\n",
    "                                            .groupby('advertiser_id').agg(F.sum('clicked').alias('clicks'), \n",
    "                                                                          F.count('*').alias('views'),\n",
    "                                                                          F.countDistinct('ad_id').alias('distinct_ad_ids')) \\\n",
    "                                         .withColumn('ctr', ctr_udf('clicks','views'))\n",
    "        \n",
    "advertiser_id_popularity = advertiser_id_popularity_df.filter('views > 10 and advertiser_id is not null').select('advertiser_id', 'ctr', 'views', 'distinct_ad_ids') \\\n",
    "                            .rdd.map(lambda x: (x['advertiser_id'], (x['ctr'], x['views'], x['distinct_ad_ids'], 1))).collectAsMap()\n",
    "len(advertiser_id_popularity)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "advertiser_id_popularity_broad = sc.broadcast(advertiser_id_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#advertiser_id_popularity_df.count()\n",
    "##4063"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#get_percentiles(advertiser_id_popularity_df, 'clicks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get_percentiles(advertiser_id_popularity_df, 'views')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#advertiser_id_popularity = advertiser_id_popularity_df.filter('views > 100 and advertiser_id is not null').select('advertiser_id', 'ctr').rdd.collectAsMap()\n",
    "#len(advertiser_id_popularity)\n",
    "##3129"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Average CTR by campaign_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25270"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "campaign_id_popularity_df = train_set_df.select('clicked', 'campaign_id', 'ad_id') \\\n",
    "                                            .groupby('campaign_id').agg(F.sum('clicked').alias('clicks'), \n",
    "                                                                        F.count('*').alias('views'),\n",
    "                                                                        F.countDistinct('ad_id').alias('distinct_ad_ids')) \\\n",
    "                                         .withColumn('ctr', ctr_udf('clicks','views'))\n",
    "        \n",
    "campaign_id_popularity = campaign_id_popularity_df.filter('views > 10 and campaign_id is not null').select('campaign_id', 'ctr', 'views', 'distinct_ad_ids') \\\n",
    "                            .rdd.map(lambda x: (x['campaign_id'], (x['ctr'], x['views'], x['distinct_ad_ids'], 1))).collectAsMap()\n",
    "len(campaign_id_popularity)               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "campaign_id_popularity_broad = sc.broadcast(campaign_id_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#campaign_id_popularity_df.count()\n",
    "##31390"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#get_percentiles(campaign_id_popularity_df, 'clicks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get_percentiles(campaign_id_popularity_df, 'views')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#campaign_id_popularity = campaign_id_popularity_df.filter('views > 100 and campaign_id is not null').select('campaign_id', 'ctr').rdd.collectAsMap()\n",
    "#len(campaign_id_popularity)\n",
    "##16097"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Average CTR by category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "category_id_popularity_df = train_set_df.join(documents_categories_df.alias('cat_local'), on=F.col(\"document_id_promo\") == F.col(\"cat_local.document_id_cat\"), how='inner') \\\n",
    "                                        .select('clicked', 'category_id', 'confidence_level_cat', 'ad_id') \\\n",
    "                                        .groupby('category_id').agg(F.sum('clicked').alias('clicks'), \n",
    "                                                                    F.count('*').alias('views'),\n",
    "                                                                    F.mean('confidence_level_cat').alias('avg_confidence_level_cat'),\n",
    "                                                                    F.countDistinct('ad_id').alias('distinct_ad_ids')) \\\n",
    "                                         .withColumn('ctr', ctr_udf('clicks','views'))\n",
    "            \n",
    "category_id_popularity = category_id_popularity_df.filter('views > 10').select('category_id', 'ctr', 'views', 'avg_confidence_level_cat', 'distinct_ad_ids') \\\n",
    "                            .rdd.map(lambda x: (x['category_id'], (x['ctr'], x['views'], x['distinct_ad_ids'], x['avg_confidence_level_cat']))).collectAsMap()\n",
    "len(category_id_popularity)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "category_id_popularity_broad = sc.broadcast(category_id_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.20317520201206207, 2629381, 25586, 0.4870458696523287),\n",
       " (0.2013949453830719, 10323, 44, 0.5),\n",
       " (0.2029319703578949, 1008811, 9187, 0.3881761040784007),\n",
       " (0.1313016414642334, 727310, 5295, 0.2439268331086716),\n",
       " (0.17418651282787323, 920519, 6850, 0.5333781843415426),\n",
       " (0.25920549035072327, 78513, 610, 0.5965200782357787),\n",
       " (0.2357751578092575, 692503, 4638, 0.5506155645951468),\n",
       " (0.24124911427497864, 178782, 2234, 0.5543352849187084),\n",
       " (0.21674630045890808, 1220676, 6280, 0.4995168007977859),\n",
       " (0.19554254412651062, 2172586, 13408, 0.4344011978646628)]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(category_id_popularity.values())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "692503.0"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(np.array(map(lambda x: x[1], category_id_popularity.values())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1246354.6736842105"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(map(lambda x: x[1], category_id_popularity.values())) / float(len(category_id_popularity))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Parece haver uma hierarquia nas categorias pelo padrão dos códigos...\n",
    "#category_id_popularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Average CTR by (country, category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10987"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_id_by_country_popularity_df = train_set_df.join(documents_categories_df.alias('cat_local'), on=F.col(\"document_id_promo\") == F.col(\"cat_local.document_id_cat\"), how='inner') \\\n",
    "                                        .select('clicked', 'category_id', 'confidence_level_cat', 'event_country', 'ad_id') \\\n",
    "                                        .groupby('event_country','category_id').agg(F.sum('clicked').alias('clicks'), \n",
    "                                                                                    F.count('*').alias('views'),\n",
    "                                                                                    F.mean('confidence_level_cat').alias('avg_confidence_level_cat'),\n",
    "                                                                                    F.countDistinct('ad_id').alias('distinct_ad_ids')) \\\n",
    "                                         .withColumn('ctr', ctr_udf('clicks','views'))\n",
    "\n",
    "category_id_by_country_popularity = category_id_by_country_popularity_df.filter('views > 10 and event_country <> \"\"').select('event_country', 'category_id', 'ctr', 'views', 'avg_confidence_level_cat', 'distinct_ad_ids') \\\n",
    "                                     .rdd.map(lambda x: ((x['event_country'], x['category_id']), (x['ctr'], x['views'], x['distinct_ad_ids'], x['avg_confidence_level_cat']))).collectAsMap()\n",
    "len(category_id_by_country_popularity)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "category_id_by_country_popularity_broad = sc.broadcast(category_id_by_country_popularity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Average CTR by Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_id_popularity_df = train_set_df.join(documents_topics_df.alias('top_local'), on=F.col(\"document_id_promo\") == F.col(\"top_local.document_id_top\"), how='inner') \\\n",
    "                                        .select('clicked', 'topic_id', 'confidence_level_top', 'ad_id') \\\n",
    "                                        .groupby('topic_id').agg(F.sum('clicked').alias('clicks'), \n",
    "                                                                 F.count('*').alias('views'),\n",
    "                                                                 F.mean('confidence_level_top').alias('avg_confidence_level_top'),\n",
    "                                                                 F.countDistinct('ad_id').alias('distinct_ad_ids')) \\\n",
    "                                         .withColumn('ctr', ctr_udf('clicks','views'))\n",
    "topic_id_popularity = topic_id_popularity_df.filter('views > 10').select('topic_id', 'ctr', 'views', 'avg_confidence_level_top', 'distinct_ad_ids') \\\n",
    "                            .rdd.map(lambda x: (x['topic_id'], (x['ctr'], x['views'], x['distinct_ad_ids'], x['avg_confidence_level_top']))).collectAsMap()\n",
    "len(topic_id_popularity)                                                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "topic_id_popularity_broad = sc.broadcast(topic_id_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "526640.4966666667"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(map(lambda x: x[1], topic_id_popularity.values())) / float(len(topic_id_popularity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6998657300.896667"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(map(lambda x: x[2]*x[1], topic_id_popularity.values())) / float(len(topic_id_popularity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Average CTR by (country, topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33071"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_id_by_country_popularity_df = train_set_df.join(documents_topics_df.alias('top_local'), on=F.col(\"document_id_promo\") == F.col(\"top_local.document_id_top\"), how='inner') \\\n",
    "                                        .select('clicked', 'topic_id', 'confidence_level_top','event_country', 'ad_id') \\\n",
    "                                        .groupby('event_country','topic_id').agg(F.sum('clicked').alias('clicks'), \n",
    "                                                                             F.count('*').alias('views'),\n",
    "                                                                             F.mean('confidence_level_top').alias('avg_confidence_level_top'),\n",
    "                                                                             F.countDistinct('ad_id').alias('distinct_ad_ids')) \\\n",
    "                                         .withColumn('ctr', ctr_udf('clicks','views'))\n",
    "            \n",
    "topic_id_id_by_country_popularity = topic_id_by_country_popularity_df.filter('views > 10 and event_country <> \"\"').select('event_country', 'topic_id', 'ctr', 'views', 'avg_confidence_level_top', 'distinct_ad_ids') \\\n",
    "                            .rdd.map(lambda x: ((x['event_country'], x['topic_id']), (x['ctr'], x['views'], x['distinct_ad_ids'], x['avg_confidence_level_top']))).collectAsMap()\n",
    "len(topic_id_id_by_country_popularity)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "topic_id_id_by_country_popularity_broad = sc.broadcast(topic_id_id_by_country_popularity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Average CTR by Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78120"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_id_popularity_df = train_set_df.join(documents_entities_df.alias('ent_local'), on=F.col(\"document_id_promo\") == F.col(\"ent_local.document_id_ent\"), how='inner') \\\n",
    "                                        .select('clicked', 'entity_id', 'confidence_level_ent', 'ad_id') \\\n",
    "                                        .groupby('entity_id').agg(F.sum('clicked').alias('clicks'), \n",
    "                                                                  F.count('*').alias('views'),\n",
    "                                                                  F.mean('confidence_level_ent').alias('avg_confidence_level_ent'),\n",
    "                                                                  F.countDistinct('ad_id').alias('distinct_ad_ids')) \\\n",
    "                                         .withColumn('ctr', ctr_udf('clicks','views'))\n",
    "\n",
    "entity_id_popularity = entity_id_popularity_df.filter('views > 5').select('entity_id', 'ctr', 'views', 'avg_confidence_level_ent', 'distinct_ad_ids') \\\n",
    "                                     .rdd.map(lambda x: (x['entity_id'], (x['ctr'], x['views'], x['distinct_ad_ids'], x['avg_confidence_level_ent']))).collectAsMap()\n",
    "len(entity_id_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "entity_id_popularity_broad = sc.broadcast(entity_id_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48.0"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(np.array(map(lambda x: x[1], entity_id_popularity.values())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1915.9761776753712"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(map(lambda x: x[1], entity_id_popularity.values())) / float(len(entity_id_popularity))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Average CTR by (country, entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "217703"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_id_by_country_popularity_df = train_set_df.join(documents_entities_df.alias('ent_local'), on=F.col(\"document_id_promo\") == F.col(\"ent_local.document_id_ent\"), how='inner') \\\n",
    "                                        .select('clicked', 'entity_id', 'event_country', 'confidence_level_ent','ad_id') \\\n",
    "                                        .groupby('event_country','entity_id').agg(F.sum('clicked').alias('clicks'), \n",
    "                                                                             F.count('*').alias('views'),\n",
    "                                                                             F.mean('confidence_level_ent').alias('avg_confidence_level_ent'),\n",
    "                                                                             F.countDistinct('ad_id').alias('distinct_ad_ids')) \\\n",
    "                                         .withColumn('ctr', ctr_udf('clicks','views'))\n",
    "            \n",
    "entity_id_by_country_popularity = entity_id_by_country_popularity_df.filter('views > 5 and event_country <> \"\"').select('event_country', 'entity_id', 'ctr', 'views', 'avg_confidence_level_ent', 'distinct_ad_ids') \\\n",
    "                .rdd.map(lambda x: ((x['event_country'], x['entity_id']), (x['ctr'], x['views'], x['distinct_ad_ids'], x['avg_confidence_level_ent']))).collectAsMap()\n",
    "len(entity_id_by_country_popularity)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "entity_id_by_country_popularity_broad = sc.broadcast(entity_id_by_country_popularity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Loading # docs by categories, topics, entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import cPickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_filenames_suffix = ''\n",
    "if evaluation:\n",
    "    df_filenames_suffix = '_eval'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('aux_data/categories_docs_counts'+df_filenames_suffix+'.pickle', 'rb') as input_file:\n",
    "    categories_docs_counts = cPickle.load(input_file)    \n",
    "len(categories_docs_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('aux_data/topics_docs_counts'+df_filenames_suffix+'.pickle', 'rb') as input_file:\n",
    "    topics_docs_counts = cPickle.load(input_file)\n",
    "len(topics_docs_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1326009"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('aux_data/entities_docs_counts'+df_filenames_suffix+'.pickle', 'rb') as input_file:\n",
    "    entities_docs_counts = cPickle.load(input_file)\n",
    "len(entities_docs_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2999334"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_total = documents_meta_df.count()\n",
    "documents_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Exploring Publish Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.5: 1464116400.0}"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "publish_times_df = train_set_df.filter('publish_time is not null').select('document_id_promo','publish_time').distinct().select(F.col('publish_time').cast(IntegerType()))\n",
    "publish_time_percentiles = get_percentiles(publish_times_df, 'publish_time', quantiles_levels=[0.5], max_error_rate=0.001)\n",
    "publish_time_percentiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2016, 5, 24, 19, 0)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "publish_time_median = int(publish_time_percentiles[0.5])\n",
    "datetime.datetime.utcfromtimestamp(publish_time_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_days_diff(newer_timestamp, older_timestamp):\n",
    "    sec_diff = newer_timestamp - older_timestamp\n",
    "    days_diff = sec_diff / 60 / 60 / 24\n",
    "    return days_diff\n",
    "\n",
    "def get_time_decay_factor(timestamp, timestamp_ref=None, alpha=0.001):\n",
    "    if timestamp_ref == None:\n",
    "        timestamp_ref = time.time()\n",
    "        \n",
    "    days_diff = get_days_diff(timestamp_ref, timestamp)\n",
    "    denominator = math.pow(1+alpha, days_diff)\n",
    "    if denominator != 0:\n",
    "        return 1.0 / denominator\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def convert_odd_timestamp(timestamp_ms_relative):\n",
    "    TIMESTAMP_DELTA=1465876799998\n",
    "    return datetime.datetime.fromtimestamp((int(timestamp_ms_relative)+TIMESTAMP_DELTA)//1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "TIME_DECAY_ALPHA = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(datetime.datetime(2016, 10, 17, 14, 34, 40), 0.9327197659467544)\n",
      "(datetime.datetime(2016, 9, 24, 14, 34, 40), 0.9220575790110879)\n",
      "(datetime.datetime(2016, 7, 24, 14, 34, 40), 0.8939192253574164)\n",
      "(datetime.datetime(2016, 4, 24, 14, 34, 40), 0.8541670558020203)\n",
      "(datetime.datetime(2015, 10, 24, 14, 34, 40), 0.7794976375182161)\n",
      "(datetime.datetime(2014, 10, 24, 14, 34, 40), 0.6494950842208255)\n"
     ]
    }
   ],
   "source": [
    "ref_dates = [\n",
    "                1476714880, # 7 days\n",
    "                1474727680, # 30 days\n",
    "                1469370880, # 90 days\n",
    "                1461508480,  # 180 days\n",
    "                1445697280, # 1 year\n",
    "                1414161280 # 2 years\n",
    "]\n",
    "\n",
    "for d in ref_dates:\n",
    "    print(datetime.datetime.utcfromtimestamp(d), get_time_decay_factor(d, alpha=TIME_DECAY_ALPHA))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Get local time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "DEFAULT_TZ_EST = -4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_local_utc_bst_tz(event_country, event_country_state):\n",
    "    local_tz = DEFAULT_TZ_EST\n",
    "    if len(event_country) > 0:\n",
    "        if event_country in countries_utc_dst_broad.value:\n",
    "            local_tz = countries_utc_dst_broad.value[event_country]\n",
    "            if len(event_country_state)>2:\n",
    "                state = event_country_state[3:5]\n",
    "                if event_country == 'US':  \n",
    "                    if state in us_states_utc_dst_broad.value:\n",
    "                        local_tz = us_states_utc_dst_broad.value[state]                \n",
    "                elif event_country == 'CA':\n",
    "                    if state in ca_countries_utc_dst_broad.value:\n",
    "                        local_tz = ca_countries_utc_dst_broad.value[state] \n",
    "    return local_tz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "hour_bins_dict = {'EARLY_MORNING': 1,\n",
    "             'MORNING': 2,\n",
    "             'MIDDAY': 3,\n",
    "             'AFTERNOON': 4,\n",
    "             'EVENING': 5,\n",
    "             'NIGHT': 6}\n",
    "\n",
    "hour_bins_values = sorted(hour_bins_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_hour_bin(hour):\n",
    "    if hour >= 5 and hour < 8:\n",
    "        hour_bin = hour_bins_dict['EARLY_MORNING']\n",
    "    elif hour >= 8 and hour < 11:\n",
    "        hour_bin = hour_bins_dict['MORNING']\n",
    "    elif hour >= 11 and hour < 14:\n",
    "        hour_bin = hour_bins_dict['MIDDAY']\n",
    "    elif hour >= 14 and hour < 19:\n",
    "        hour_bin = hour_bins_dict['AFTERNOON']\n",
    "    elif hour >= 19 and hour < 22:\n",
    "        hour_bin = hour_bins_dict['EVENING']\n",
    "    else:\n",
    "        hour_bin = hour_bins_dict['NIGHT']\n",
    "    return hour_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_local_datetime(dt, event_country, event_country_state):\n",
    "    local_tz = get_local_utc_bst_tz(event_country, event_country_state)    \n",
    "    tz_delta = local_tz - DEFAULT_TZ_EST\n",
    "    local_time = dt +  datetime.timedelta(hours=tz_delta)\n",
    "    return local_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2017, 3, 5, 19, 38, 11, 879128)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_local_datetime(datetime.datetime.now(), 'US', 'US>CA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def is_weekend(dt):\n",
    "    return dt.weekday() >= 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_weekend(datetime.datetime(2016, 6, 14))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Average CTR functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('decay_factor_default', 0.9826565333455068)\n"
     ]
    }
   ],
   "source": [
    "timestamp_ref = date_time_to_unix_epoch(datetime.datetime(2016, 6, 29, 3, 59, 59))\n",
    "decay_factor_default = get_time_decay_factor(publish_time_median, timestamp_ref, alpha=TIME_DECAY_ALPHA)\n",
    "print(\"decay_factor_default\", decay_factor_default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0.0)\n",
      "(0.5, 0.024411410743763327)\n",
      "(1, 0.041731582304281624)\n",
      "(2, 0.06614299304804495)\n",
      "(3, 0.08346316460856325)\n",
      "(4, 0.09689773339641579)\n",
      "(5, 0.10787457535232657)\n",
      "(10, 0.14436755531919657)\n",
      "(20, 0.183298356035222)\n",
      "(30, 0.20674645107847822)\n",
      "(100, 0.2778577004917695)\n",
      "(200, 0.3192904933647466)\n",
      "(300, 0.34360197720285013)\n",
      "(1000, 0.41594812296601125)\n",
      "(2000, 0.4576496248565576)\n",
      "(3000, 0.48205100545505175)\n",
      "(10000, 0.5545232830964639)\n",
      "(20000, 0.5962518553291584)\n",
      "(30000, 0.6206622626822822)\n",
      "(50000, 0.6514162003061013)\n",
      "(90000, 0.6868039178501281)\n",
      "(100000, 1.0)\n",
      "(500000, 1.0)\n",
      "(900000, 1.0)\n",
      "(1000000, 1.0)\n",
      "(2171607, 1.0)\n"
     ]
    }
   ],
   "source": [
    "def get_confidence_sample_size(sample, max_for_reference=100000):\n",
    "    #Avoiding overflow for large sample size\n",
    "    if sample >= max_for_reference:\n",
    "        return 1.0\n",
    "\n",
    "    ref_log = math.log(1+max_for_reference, 2) #Curiosly reference in log  with base 2 gives a slightly higher score, so I will keep\n",
    "    \n",
    "    return math.log(1+sample) / float(ref_log)\n",
    "    \n",
    "for i in [0,0.5,1,2,3,4,5,10,20,30,100,200,300,1000,2000,3000,10000,20000,30000, 50000, 90000, 100000, 500000, 900000, 1000000, 2171607]:\n",
    "    print(i, get_confidence_sample_size(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_popularity(an_id, a_dict):\n",
    "    return (a_dict[an_id][0], get_confidence_sample_size(a_dict[an_id][1] / float(a_dict[an_id][2])) * a_dict[an_id][3]) if an_id in a_dict else (None, None)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_weighted_avg_popularity_from_list(ids_list, confidence_ids_list, pop_dict):\n",
    "    pops = filter(lambda x: x[0][0]!=None, [(get_popularity(an_id, pop_dict), confidence) for an_id, confidence in zip(ids_list, confidence_ids_list)])\n",
    "    #print(\"pops\",pops)\n",
    "    if len(pops) > 0:\n",
    "        weighted_avg = sum(map(lambda x: x[0][0]*x[0][1]*x[1], pops)) / float(sum(map(lambda x: x[0][1]*x[1], pops)))\n",
    "        confidence = max(map(lambda x: x[0][1]*x[1], pops))\n",
    "        return weighted_avg, confidence\n",
    "    else:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_weighted_avg_country_popularity_from_list(event_country, ids_list, confidence_ids_list, pop_dict):\n",
    "    pops = filter(lambda x: x[0][0]!=None, [(get_popularity((event_country, an_id), pop_dict), confidence) for an_id, confidence in zip(ids_list, confidence_ids_list)])\n",
    "    \n",
    "    if len(pops) > 0:\n",
    "        weighted_avg = sum(map(lambda x: x[0][0]*x[0][1]*x[1], pops)) / float(sum(map(lambda x: x[0][1]*x[1], pops)))\n",
    "        confidence = max(map(lambda x: x[0][1]*x[1], pops))\n",
    "        return weighted_avg, confidence\n",
    "    else:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_popularity_score(event_country, ad_id, document_id, source_id, \n",
    "                         publisher_id, advertiser_id, campaign_id, document_id_event,\n",
    "                            category_ids_by_doc, cat_confidence_level_by_doc, \n",
    "                            topic_ids_by_doc, top_confidence_level_by_doc,\n",
    "                            entity_ids_by_doc, ent_confidence_level_by_doc,\n",
    "                            output_detailed_list=False):\n",
    "    probs = []\n",
    "    \n",
    "    avg_ctr, confidence = get_popularity(ad_id, ad_id_popularity_broad.value)    \n",
    "    if avg_ctr != None:\n",
    "        probs.append(('pop_ad_id', avg_ctr, confidence))\n",
    "        \n",
    "    avg_ctr, confidence = get_popularity(document_id, document_id_popularity_broad.value)\n",
    "    if avg_ctr != None:\n",
    "        probs.append(('pop_document_id', avg_ctr, confidence))  \n",
    "        \n",
    "    avg_ctr, confidence = get_popularity((document_id_event, document_id), doc_event_doc_ad_avg_ctr_broad.value)\n",
    "    if avg_ctr != None:\n",
    "        probs.append(('pop_doc_event_doc_ad', avg_ctr, confidence))\n",
    "        \n",
    "        \n",
    "    if source_id != -1:\n",
    "        avg_ctr = None\n",
    "        if event_country != '':\n",
    "            avg_ctr, confidence = get_popularity((event_country, source_id), source_id_by_country_popularity_broad.value)\n",
    "        if avg_ctr != None:\n",
    "            probs.append(('pop_source_id_country', avg_ctr, confidence))\n",
    "            \n",
    "        avg_ctr, confidence = get_popularity(source_id, source_id_popularity_broad.value)        \n",
    "        if avg_ctr != None:\n",
    "            probs.append(('pop_source_id', avg_ctr, confidence))\n",
    "            \n",
    "            \n",
    "    if publisher_id != None:\n",
    "        avg_ctr, confidence = get_popularity(publisher_id, publisher_popularity_broad.value)\n",
    "        if avg_ctr != None:\n",
    "            probs.append(('pop_publisher_id', avg_ctr, confidence)) \n",
    "            \n",
    "    if advertiser_id != None:\n",
    "        avg_ctr, confidence = get_popularity(advertiser_id, advertiser_id_popularity_broad.value)\n",
    "        if avg_ctr != None:\n",
    "            probs.append(('pop_advertiser_id', avg_ctr, confidence)) \n",
    "    \n",
    "    if campaign_id != None:\n",
    "        avg_ctr, confidence = get_popularity(campaign_id, campaign_id_popularity_broad.value)\n",
    "        if avg_ctr != None:\n",
    "            probs.append(('pop_campain_id', avg_ctr, confidence))  \n",
    "\n",
    "    if len(entity_ids_by_doc) > 0: \n",
    "        avg_ctr = None\n",
    "        if event_country != '':\n",
    "            avg_ctr, confidence = get_weighted_avg_country_popularity_from_list(event_country, entity_ids_by_doc, ent_confidence_level_by_doc, \n",
    "                                        entity_id_by_country_popularity_broad.value)\n",
    "        if avg_ctr != None:\n",
    "            probs.append(('pop_entity_id_country', avg_ctr, confidence))\n",
    "            \n",
    "        avg_ctr, confidence = get_weighted_avg_popularity_from_list(entity_ids_by_doc, ent_confidence_level_by_doc, \n",
    "                                                                    entity_id_popularity_broad.value) \n",
    "        if avg_ctr != None:\n",
    "            probs.append(('pop_entity_id', avg_ctr, confidence))\n",
    "            \n",
    "    \n",
    "    \n",
    "    if len(topic_ids_by_doc) > 0:  \n",
    "        avg_ctr = None\n",
    "        if event_country != '':\n",
    "            avg_ctr, confidence = get_weighted_avg_country_popularity_from_list(event_country, topic_ids_by_doc, top_confidence_level_by_doc, \n",
    "                                        topic_id_id_by_country_popularity_broad.value)\n",
    "        if avg_ctr != None:\n",
    "            probs.append(('pop_topic_id_country', avg_ctr, confidence))\n",
    "            \n",
    "        avg_ctr, confidence = get_weighted_avg_popularity_from_list(topic_ids_by_doc, top_confidence_level_by_doc, \n",
    "                                                                    topic_id_popularity_broad.value)            \n",
    "        if avg_ctr != None:\n",
    "            probs.append(('pop_topic_id', avg_ctr, confidence))\n",
    "    \n",
    "    \n",
    "    if len(category_ids_by_doc) > 0:  \n",
    "        avg_ctr = None\n",
    "        if event_country != '':\n",
    "            avg_ctr, confidence = get_weighted_avg_country_popularity_from_list(event_country, category_ids_by_doc, cat_confidence_level_by_doc, \n",
    "                                        category_id_by_country_popularity_broad.value)\n",
    "        if avg_ctr != None:\n",
    "            probs.append(('pop_category_id_country', avg_ctr, confidence))\n",
    "        \n",
    "        avg_ctr, confidence = get_weighted_avg_popularity_from_list(category_ids_by_doc, cat_confidence_level_by_doc, \n",
    "                                                                    category_id_popularity_broad.value)\n",
    "        if avg_ctr != None:\n",
    "            probs.append(('pop_category_id', avg_ctr, confidence))\n",
    "    \n",
    "    #print(\"[get_popularity_score] probs\", probs)\n",
    "    if output_detailed_list:\n",
    "        return probs\n",
    "    \n",
    "    else:    \n",
    "        if len(probs) > 0:\n",
    "            #weighted_avg_probs_by_confidence = sum(map(lambda x: x[1] *  math.log(1+x[2],2), probs)) / float(sum(map(lambda x: math.log(1+x[2],2), probs)))        \n",
    "            weighted_avg_probs_by_confidence = sum(map(lambda x: x[1] * x[2], probs)) / float(sum(map(lambda x: x[2], probs)))                \n",
    "            confidence = max(map(lambda x: x[2], probs))\n",
    "            return weighted_avg_probs_by_confidence, confidence\n",
    "        else:\n",
    "            return None, None    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Content-Based similarity functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def cosine_similarity_dicts(dict1, dict2):\n",
    "    dict1_norm = math.sqrt(sum([v**2 for v in dict1.values()]))\n",
    "    dict2_norm = math.sqrt(sum([v**2 for v in dict2.values()]))\n",
    "    \n",
    "    sum_common_aspects = 0.0\n",
    "    intersections = 0\n",
    "    for key in dict1:\n",
    "        if key in dict2:\n",
    "            sum_common_aspects += dict1[key] * dict2[key] \n",
    "            intersections += 1\n",
    "        \n",
    "    return sum_common_aspects / (dict1_norm * dict2_norm), intersections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def cosine_similarity_user_docs_aspects(user_aspect_profile, doc_aspect_ids, doc_aspects_confidence, aspect_docs_counts):\n",
    "    if user_aspect_profile==None or len(user_aspect_profile) == 0 or doc_aspect_ids == None or len(doc_aspect_ids) == 0:\n",
    "        return None, None\n",
    "        \n",
    "    doc_aspects = dict(zip(doc_aspect_ids, doc_aspects_confidence))\n",
    "    doc_aspects_tfidf_confid = {}\n",
    "    for key in doc_aspects:\n",
    "        tf = 1.0\n",
    "        idf = math.log(math.log(documents_total / float(aspect_docs_counts[key])))\n",
    "        confidence = doc_aspects[key]\n",
    "        doc_aspects_tfidf_confid[key] = tf*idf * confidence\n",
    "        \n",
    "    user_aspects_tfidf_confid = {}    \n",
    "    for key in user_aspect_profile:\n",
    "        tfidf = user_aspect_profile[key][0]\n",
    "        confidence = user_aspect_profile[key][1]\n",
    "        user_aspects_tfidf_confid[key] = tfidf * confidence\n",
    "        \n",
    "    similarity, intersections = cosine_similarity_dicts(doc_aspects_tfidf_confid, user_aspects_tfidf_confid)\n",
    "    \n",
    "    if intersections > 0:\n",
    "        #P(A intersect B)_intersections = P(A)^intersections * P(B)^intersections\n",
    "        random_error = math.pow(len(doc_aspects)         / float(len(aspect_docs_counts)), intersections) * \\\n",
    "                       math.pow(len(user_aspect_profile) / float(len(aspect_docs_counts)), intersections)\n",
    "        confidence = 1.0 - random_error\n",
    "    else:\n",
    "        #P(A not intersect B) = 1 - P(A intersect B)\n",
    "        random_error = 1 - ((len(doc_aspects) / float(len(aspect_docs_counts))) * \\\n",
    "                            (len(user_aspect_profile) / float(len(aspect_docs_counts))))\n",
    "    \n",
    "    confidence = 1.0 - random_error    \n",
    "    \n",
    "    return similarity, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def cosine_similarity_doc_event_doc_ad_aspects(doc_event_aspect_ids, doc_event_aspects_confidence, \n",
    "                                               doc_ad_aspect_ids, doc_ad_aspects_confidence, \n",
    "                                               aspect_docs_counts):\n",
    "    if doc_event_aspect_ids == None or len(doc_event_aspect_ids) == 0 or \\\n",
    "       doc_ad_aspect_ids == None or len(doc_ad_aspect_ids) == 0:\n",
    "        return None, None\n",
    "        \n",
    "    doc_event_aspects = dict(zip(doc_event_aspect_ids, doc_event_aspects_confidence))\n",
    "    doc_event_aspects_tfidf_confid = {}\n",
    "    for key in doc_event_aspect_ids:\n",
    "        tf = 1.0\n",
    "        idf = math.log(math.log(documents_total / float(aspect_docs_counts[key])))\n",
    "        confidence = doc_event_aspects[key]\n",
    "        doc_event_aspects_tfidf_confid[key] = tf*idf * confidence\n",
    "        \n",
    "    doc_ad_aspects = dict(zip(doc_ad_aspect_ids, doc_ad_aspects_confidence))\n",
    "    doc_ad_aspects_tfidf_confid = {}\n",
    "    for key in doc_ad_aspect_ids:\n",
    "        tf = 1.0\n",
    "        idf = math.log(math.log(documents_total / float(aspect_docs_counts[key])))\n",
    "        confidence = doc_ad_aspects[key]\n",
    "        doc_ad_aspects_tfidf_confid[key] = tf*idf * confidence\n",
    "        \n",
    "    similarity, intersections = cosine_similarity_dicts(doc_event_aspects_tfidf_confid, doc_ad_aspects_tfidf_confid)\n",
    "    \n",
    "    if intersections > 0:\n",
    "        #P(A intersect B)_intersections = P(A)^intersections * P(B)^intersections\n",
    "        random_error = math.pow(len(doc_event_aspect_ids) / float(len(aspect_docs_counts)), intersections) * \\\n",
    "                       math.pow(len(doc_ad_aspect_ids) / float(len(aspect_docs_counts)), intersections)\n",
    "        confidence = 1.0 - random_error\n",
    "    else:\n",
    "        #P(A not intersect B) = 1 - P(A intersect B)\n",
    "        random_error = 1 - ((len(doc_event_aspect_ids) / float(len(aspect_docs_counts))) * \\\n",
    "                            (len(doc_ad_aspect_ids) / float(len(aspect_docs_counts))))\n",
    "    \n",
    "    confidence = 1.0 - random_error    \n",
    "    \n",
    "    return similarity, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_user_cb_interest_score(user_views_count, user_categories, user_topics, user_entities, \n",
    "                            timestamp_event, category_ids_by_doc, cat_confidence_level_by_doc, \n",
    "                            topic_ids_by_doc, top_confidence_level_by_doc,\n",
    "                            entity_ids_by_doc, ent_confidence_level_by_doc, \n",
    "                            output_detailed_list=False):\n",
    "\n",
    "    #Content-Based\n",
    "    \n",
    "    sims = []\n",
    "    \n",
    "    categories_similarity, cat_sim_confidence = cosine_similarity_user_docs_aspects(user_categories, category_ids_by_doc, cat_confidence_level_by_doc, categories_docs_counts)\n",
    "    if categories_similarity != None:\n",
    "        sims.append(('user_doc_ad_sim_categories', categories_similarity, cat_sim_confidence))\n",
    "    \n",
    "    topics_similarity, top_sim_confidence = cosine_similarity_user_docs_aspects(user_topics, topic_ids_by_doc, top_confidence_level_by_doc, topics_docs_counts)\n",
    "    if topics_similarity != None:\n",
    "        sims.append(('user_doc_ad_sim_topics', topics_similarity, top_sim_confidence))\n",
    "    \n",
    "    entities_similarity, entity_sim_confid = cosine_similarity_user_docs_aspects(user_entities, entity_ids_by_doc, ent_confidence_level_by_doc, entities_docs_counts)\n",
    "    if entities_similarity != None:\n",
    "        sims.append(('user_doc_ad_sim_entities', entities_similarity, entity_sim_confid))\n",
    "    \n",
    "    if output_detailed_list:\n",
    "        return sims\n",
    "    else:\n",
    "        if len(sims) > 0:\n",
    "            weighted_avg_sim_by_confidence = sum(map(lambda x: x[1]*x[2], sims)) / float(sum(map(lambda x: x[2], sims)))\n",
    "            confidence = sum(map(lambda x: x[2], sims)) / float(len(sims))\n",
    "\n",
    "            #print(\"[get_user_cb_interest_score] sims: {} | Avg: {} - Confid: {}\".format(sims, weighted_avg_sim_by_confidence, confidence))\n",
    "            return weighted_avg_sim_by_confidence, confidence\n",
    "        else:\n",
    "            return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_doc_event_doc_ad_cb_similarity_score(doc_event_category_ids, doc_event_cat_confidence_levels, \n",
    "                                             doc_event_topic_ids, doc_event_top_confidence_levels,\n",
    "                                             doc_event_entity_ids, doc_event_ent_confidence_levels, \n",
    "                                             doc_ad_category_ids, doc_ad_cat_confidence_levels, \n",
    "                                             doc_ad_topic_ids, doc_ad_top_confidence_levels,\n",
    "                                             doc_ad_entity_ids, doc_ad_ent_confidence_levels,\n",
    "                            output_detailed_list=False):\n",
    "\n",
    "    #Content-Based\n",
    "    sims = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    categories_similarity, cat_sim_confidence = cosine_similarity_doc_event_doc_ad_aspects(\n",
    "                                                    doc_event_category_ids, doc_event_cat_confidence_levels, \n",
    "                                                    doc_ad_category_ids, doc_ad_cat_confidence_levels, \n",
    "                                                    categories_docs_counts)\n",
    "    if categories_similarity != None:\n",
    "        sims.append(('doc_event_doc_ad_sim_categories', categories_similarity, cat_sim_confidence))\n",
    "    \n",
    "    topics_similarity, top_sim_confidence = cosine_similarity_doc_event_doc_ad_aspects(\n",
    "                                                    doc_event_topic_ids, doc_event_top_confidence_levels, \n",
    "                                                    doc_ad_topic_ids, doc_ad_top_confidence_levels, \n",
    "                                                    topics_docs_counts)\n",
    "    \n",
    "    if topics_similarity != None:\n",
    "        sims.append(('doc_event_doc_ad_sim_topics', topics_similarity, top_sim_confidence))\n",
    "        \n",
    "    entities_similarity, entity_sim_confid = cosine_similarity_doc_event_doc_ad_aspects(\n",
    "                                                    doc_event_entity_ids, doc_event_ent_confidence_levels, \n",
    "                                                    doc_ad_entity_ids, doc_ad_ent_confidence_levels, \n",
    "                                                    entities_docs_counts)\n",
    "    \n",
    "    if entities_similarity != None:\n",
    "        sims.append(('doc_event_doc_ad_sim_entities', entities_similarity, entity_sim_confid))\n",
    "    \n",
    "    if output_detailed_list:\n",
    "        return sims\n",
    "    else:\n",
    "        if len(sims) > 0:\n",
    "            weighted_avg_sim_by_confidence = sum(map(lambda x: x[1]*x[2], sims)) / float(sum(map(lambda x: x[2], sims)))\n",
    "            confidence = sum(map(lambda x: x[2], sims)) / float(len(sims))\n",
    "\n",
    "            #print(\"[get_user_cb_interest_score] sims: {} | Avg: {} - Confid: {}\".format(sims, weighted_avg_sim_by_confidence, confidence))\n",
    "            return weighted_avg_sim_by_confidence, confidence\n",
    "        else:\n",
    "            return None, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Feature Vector export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "bool_feature_names = ['event_weekend',\n",
    "                      'user_has_already_viewed_doc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "int_feature_names = ['user_views',\n",
    "                    'ad_views',\n",
    "                    'doc_views',\n",
    "                    'doc_event_days_since_published',\n",
    "                    'doc_event_hour',\n",
    "                    'doc_ad_days_since_published', \n",
    "                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "float_feature_names = [                                \n",
    "                'pop_ad_id',       \n",
    "                'pop_ad_id_conf',   \n",
    "                'pop_ad_id_conf_multipl', \n",
    "                'pop_document_id',                \n",
    "                'pop_document_id_conf',\n",
    "                'pop_document_id_conf_multipl',\n",
    "                'pop_publisher_id',\n",
    "                'pop_publisher_id_conf',\n",
    "                'pop_publisher_id_conf_multipl',\n",
    "                'pop_advertiser_id',\n",
    "                'pop_advertiser_id_conf',\n",
    "                'pop_advertiser_id_conf_multipl',\n",
    "                'pop_campain_id',\n",
    "                'pop_campain_id_conf',\n",
    "                'pop_campain_id_conf_multipl',\n",
    "                'pop_doc_event_doc_ad',\n",
    "                'pop_doc_event_doc_ad_conf',\n",
    "                'pop_doc_event_doc_ad_conf_multipl',\n",
    "                'pop_source_id',  \n",
    "                'pop_source_id_conf',\n",
    "                'pop_source_id_conf_multipl',\n",
    "                'pop_source_id_country',\n",
    "                'pop_source_id_country_conf',\n",
    "                'pop_source_id_country_conf_multipl',\n",
    "                'pop_entity_id',    \n",
    "                'pop_entity_id_conf',\n",
    "                'pop_entity_id_conf_multipl',\n",
    "                'pop_entity_id_country',\n",
    "                'pop_entity_id_country_conf',\n",
    "                'pop_entity_id_country_conf_multipl',\n",
    "                'pop_topic_id', \n",
    "                'pop_topic_id_conf',\n",
    "                'pop_topic_id_conf_multipl',\n",
    "                'pop_topic_id_country',\n",
    "                'pop_topic_id_country_conf',\n",
    "                'pop_topic_id_country_conf_multipl',\n",
    "                'pop_category_id', \n",
    "                'pop_category_id_conf',\n",
    "                'pop_category_id_conf_multipl',\n",
    "                'pop_category_id_country',\n",
    "                'pop_category_id_country_conf',\n",
    "                'pop_category_id_country_conf_multipl',\n",
    "                'user_doc_ad_sim_categories',    \n",
    "                'user_doc_ad_sim_categories_conf',\n",
    "                'user_doc_ad_sim_categories_conf_multipl',\n",
    "                'user_doc_ad_sim_topics',    \n",
    "                'user_doc_ad_sim_topics_conf',\n",
    "                'user_doc_ad_sim_topics_conf_multipl',\n",
    "                'user_doc_ad_sim_entities',                    \n",
    "                'user_doc_ad_sim_entities_conf',\n",
    "                'user_doc_ad_sim_entities_conf_multipl',\n",
    "                'doc_event_doc_ad_sim_categories',    \n",
    "                'doc_event_doc_ad_sim_categories_conf',\n",
    "                'doc_event_doc_ad_sim_categories_conf_multipl',\n",
    "                'doc_event_doc_ad_sim_topics',    \n",
    "                'doc_event_doc_ad_sim_topics_conf',\n",
    "                'doc_event_doc_ad_sim_topics_conf_multipl',\n",
    "                'doc_event_doc_ad_sim_entities',                    \n",
    "                'doc_event_doc_ad_sim_entities_conf',\n",
    "                'doc_event_doc_ad_sim_entities_conf_multipl'\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "TRAFFIC_SOURCE_FV='traffic_source'\n",
    "EVENT_HOUR_FV='event_hour'\n",
    "EVENT_COUNTRY_FV = 'event_country'\n",
    "EVENT_COUNTRY_STATE_FV = 'event_country_state'\n",
    "EVENT_GEO_LOCATION_FV = 'event_geo_location'\n",
    "EVENT_PLATFORM_FV = 'event_platform'\n",
    "AD_ADVERTISER_FV = 'ad_advertiser'\n",
    "DOC_AD_SOURCE_ID_FV='doc_ad_source_id'\n",
    "DOC_AD_PUBLISHER_ID_FV='doc_ad_publisher_id'\n",
    "DOC_EVENT_SOURCE_ID_FV='doc_event_source_id'\n",
    "DOC_EVENT_PUBLISHER_ID_FV='doc_event_publisher_id'\n",
    "DOC_AD_CATEGORY_ID_FV='doc_ad_category_id'\n",
    "DOC_AD_TOPIC_ID_FV='doc_ad_topic_id'\n",
    "DOC_AD_ENTITY_ID_FV='doc_ad_entity_id'\n",
    "DOC_EVENT_CATEGORY_ID_FV='doc_event_category_id'\n",
    "DOC_EVENT_TOPIC_ID_FV='doc_event_topic_id'\n",
    "DOC_EVENT_ENTITY_ID_FV='doc_event_entity_id'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Configuring feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "category_feature_names_integral = ['ad_advertiser',\n",
    " 'doc_ad_category_id_1',\n",
    " 'doc_ad_category_id_2',\n",
    " 'doc_ad_category_id_3',\n",
    " 'doc_ad_topic_id_1',\n",
    " 'doc_ad_topic_id_2',\n",
    " 'doc_ad_topic_id_3',\n",
    " 'doc_ad_entity_id_1', \n",
    " 'doc_ad_entity_id_2', \n",
    " 'doc_ad_entity_id_3', \n",
    " 'doc_ad_entity_id_4', \n",
    " 'doc_ad_entity_id_5', \n",
    " 'doc_ad_entity_id_6', \n",
    " 'doc_ad_publisher_id',\n",
    " 'doc_ad_source_id', \n",
    " 'doc_event_category_id_1',\n",
    " 'doc_event_category_id_2',\n",
    " 'doc_event_category_id_3',\n",
    " 'doc_event_topic_id_1',\n",
    " 'doc_event_topic_id_2',\n",
    " 'doc_event_topic_id_3',\n",
    " 'doc_event_entity_id_1',\n",
    " 'doc_event_entity_id_2',\n",
    " 'doc_event_entity_id_3',\n",
    " 'doc_event_entity_id_4',\n",
    " 'doc_event_entity_id_5',\n",
    " 'doc_event_entity_id_6',\n",
    " 'doc_event_publisher_id',\n",
    " 'doc_event_source_id', \n",
    " 'event_country',\n",
    " 'event_country_state',\n",
    " 'event_geo_location',\n",
    " 'event_hour',\n",
    " 'event_platform',\n",
    " 'traffic_source']\n",
    "\n",
    "\n",
    "feature_vector_labels_integral = bool_feature_names + int_feature_names + float_feature_names + \\\n",
    "                                 category_feature_names_integral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "feature_vector_labels_integral_dict = dict([(key, idx) for idx, key in enumerate(feature_vector_labels_integral)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open('feature_vector_labels_integral.txt', 'w') as output:\n",
    "    output.writelines('\\n'.join(feature_vector_labels_integral))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def set_feature_vector_cat_value(field_name, field_value, feature_vector):\n",
    "    if not is_null(field_value) and str(field_value) != '-1':\n",
    "        feature_name = get_ohe_feature_name(field_name, field_value)\n",
    "        if feature_name in feature_vector_labels_dict:\n",
    "            feature_idx = feature_vector_labels_dict[feature_name]\n",
    "        else:\n",
    "            #Unpopular category value\n",
    "            feature_idx = feature_vector_labels_dict[get_ohe_feature_name(field_name, LESS_SPECIAL_CAT_VALUE)]\n",
    "            \n",
    "        feature_vector[feature_idx] = float(1)\n",
    "        \n",
    "def set_feature_vector_cat_values(field_name, field_values, feature_vector):\n",
    "    for field_value in field_values:\n",
    "        set_feature_vector_cat_value(field_name, field_value, feature_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_ad_feature_vector(user_doc_ids_viewed, user_views_count, user_categories, user_topics, user_entities, \n",
    "                            event_country, event_country_state,\n",
    "                            ad_id, document_id, source_id, doc_ad_publish_time, timestamp_event, platform_event,\n",
    "                            geo_location_event, \n",
    "                            doc_event_source_id, doc_event_publisher_id, doc_event_publish_time,\n",
    "                            traffic_source_pv, advertiser_id, publisher_id,\n",
    "                            campaign_id, document_id_event,\n",
    "                            doc_ad_category_ids, doc_ad_cat_confidence_levels, \n",
    "                            doc_ad_topic_ids, doc_ad_top_confidence_levels,\n",
    "                            doc_ad_entity_ids, doc_ad_ent_confidence_levels,\n",
    "                            doc_event_category_ids, doc_event_cat_confidence_levels,\n",
    "                            doc_event_topic_ids, doc_event_top_confidence_levels,\n",
    "                            doc_event_entity_ids, doc_event_ent_confidence_levels):\n",
    "             \n",
    "    try:\n",
    "\n",
    "        feature_vector = {}\n",
    "        \n",
    "        if user_views_count != None:\n",
    "            feature_vector[feature_vector_labels_dict['user_views']] = float(user_views_count)\n",
    "         \n",
    "        if user_doc_ids_viewed != None:\n",
    "            feature_vector[feature_vector_labels_dict['user_has_already_viewed_doc']] = float(document_id in user_doc_ids_viewed)               \n",
    "          \n",
    "        if ad_id in ad_id_popularity_broad.value:            \n",
    "            feature_vector[feature_vector_labels_dict['ad_views']] = float(ad_id_popularity_broad.value[ad_id][1])\n",
    "        \n",
    "        if document_id in document_id_popularity_broad.value:\n",
    "            feature_vector[feature_vector_labels_dict['doc_views']] = float(document_id_popularity_broad.value[document_id][1])            \n",
    "            \n",
    "        if timestamp_event > -1:\n",
    "            dt_timestamp_event = convert_odd_timestamp(timestamp_event)\n",
    "            if doc_ad_publish_time != None:\n",
    "                delta_days = (dt_timestamp_event - doc_ad_publish_time).days\n",
    "                if delta_days >= 0 and delta_days <= 365*10: #10 years\n",
    "                    feature_vector[feature_vector_labels_dict['doc_ad_days_since_published']] = float(delta_days)\n",
    "                        \n",
    "            if doc_event_publish_time != None:\n",
    "                delta_days = (dt_timestamp_event - doc_event_publish_time).days\n",
    "                if delta_days >= 0 and delta_days <= 365*10: #10 years\n",
    "                    feature_vector[feature_vector_labels_dict['doc_event_days_since_published']] = float(delta_days)\n",
    "                    \n",
    "            \n",
    "            #Local period of the day (hours)\n",
    "            dt_local_timestamp_event = get_local_datetime(dt_timestamp_event, event_country, event_country_state)    \n",
    "            local_hour_bin = get_hour_bin(dt_local_timestamp_event.hour)            \n",
    "            feature_vector[feature_vector_labels_dict['doc_event_hour']] = float(local_hour_bin) #Hour for Decision Trees\n",
    "            set_feature_vector_cat_value(EVENT_HOUR_FV, local_hour_bin, feature_vector) #Period of day for FFM\n",
    "            \n",
    "            #Weekend\n",
    "            weekend = int(is_weekend(dt_local_timestamp_event))\n",
    "            feature_vector[feature_vector_labels_dict['event_weekend']] = float(weekend)                                                      \n",
    "        \n",
    "        conf_field_suffix = '_conf'\n",
    "        conf_multiplied_field_suffix = '_conf_multipl'\n",
    "        \n",
    "        #Setting Popularity fields\n",
    "        pop_scores = get_popularity_score(event_country, ad_id, document_id, source_id, \n",
    "                                publisher_id, advertiser_id, campaign_id, document_id_event,\n",
    "                                doc_ad_category_ids, doc_ad_cat_confidence_levels, \n",
    "                                doc_ad_topic_ids, doc_ad_top_confidence_levels,\n",
    "                                doc_ad_entity_ids, doc_ad_ent_confidence_levels,\n",
    "                                output_detailed_list=True)\n",
    "        \n",
    "                                \n",
    "\n",
    "        for score in pop_scores:\n",
    "            feature_vector[feature_vector_labels_dict[score[0]]] = score[1]\n",
    "            feature_vector[feature_vector_labels_dict[score[0]+conf_field_suffix]] = score[2]\n",
    "            feature_vector[feature_vector_labels_dict[score[0]+conf_multiplied_field_suffix]] = score[1] * score[2]\n",
    "\n",
    "        #Setting User-Doc_ad CB Similarity fields\n",
    "        user_doc_ad_cb_sim_scores = get_user_cb_interest_score(user_views_count, user_categories, user_topics, user_entities, \n",
    "                                timestamp_event, \n",
    "                                 doc_ad_category_ids, doc_ad_cat_confidence_levels, \n",
    "                                 doc_ad_topic_ids, doc_ad_top_confidence_levels,\n",
    "                                 doc_ad_entity_ids, doc_ad_ent_confidence_levels,\n",
    "                                output_detailed_list=True)\n",
    "\n",
    "        for score in user_doc_ad_cb_sim_scores:\n",
    "            feature_vector[feature_vector_labels_dict[score[0]]] = score[1]\n",
    "            feature_vector[feature_vector_labels_dict[score[0]+conf_field_suffix]] = score[2]\n",
    "            feature_vector[feature_vector_labels_dict[score[0]+conf_multiplied_field_suffix]] = score[1] * score[2]\n",
    "            \n",
    "        #Setting Doc_event-doc_ad CB Similarity fields\n",
    "        doc_event_doc_ad_cb_sim_scores = get_doc_event_doc_ad_cb_similarity_score(\n",
    "                                            doc_event_category_ids, doc_event_cat_confidence_levels,\n",
    "                                            doc_event_topic_ids, doc_event_top_confidence_levels,\n",
    "                                            doc_event_entity_ids, doc_event_ent_confidence_levels,\n",
    "                                            doc_ad_category_ids, doc_ad_cat_confidence_levels, \n",
    "                                            doc_ad_topic_ids, doc_ad_top_confidence_levels,\n",
    "                                            doc_ad_entity_ids, doc_ad_ent_confidence_levels,\n",
    "                                        output_detailed_list=True)\n",
    "        \n",
    "        for score in doc_event_doc_ad_cb_sim_scores:\n",
    "            feature_vector[feature_vector_labels_dict[score[0]]] = score[1]\n",
    "            feature_vector[feature_vector_labels_dict[score[0]+conf_field_suffix]] = score[2]\n",
    "            feature_vector[feature_vector_labels_dict[score[0]+conf_multiplied_field_suffix]] = score[1] * score[2]\n",
    "            \n",
    "        set_feature_vector_cat_value(TRAFFIC_SOURCE_FV, traffic_source_pv, feature_vector)\n",
    "        set_feature_vector_cat_value(EVENT_COUNTRY_FV, event_country, feature_vector)\n",
    "        set_feature_vector_cat_value(EVENT_COUNTRY_STATE_FV, event_country_state, feature_vector)         \n",
    "        set_feature_vector_cat_value(EVENT_GEO_LOCATION_FV, geo_location_event, feature_vector)\n",
    "        set_feature_vector_cat_value(EVENT_PLATFORM_FV, platform_event, feature_vector)\n",
    "        set_feature_vector_cat_value(AD_ADVERTISER_FV, advertiser_id, feature_vector)\n",
    "        set_feature_vector_cat_value(DOC_AD_SOURCE_ID_FV, source_id, feature_vector)\n",
    "        set_feature_vector_cat_value(DOC_AD_PUBLISHER_ID_FV, publisher_id, feature_vector)\n",
    "        set_feature_vector_cat_value(DOC_EVENT_SOURCE_ID_FV, doc_event_source_id, feature_vector)\n",
    "        set_feature_vector_cat_value(DOC_EVENT_PUBLISHER_ID_FV, doc_event_publisher_id, feature_vector)\n",
    "        set_feature_vector_cat_values(DOC_AD_CATEGORY_ID_FV, doc_ad_category_ids, feature_vector)\n",
    "        set_feature_vector_cat_values(DOC_AD_TOPIC_ID_FV, doc_ad_topic_ids, feature_vector)\n",
    "        set_feature_vector_cat_values(DOC_AD_ENTITY_ID_FV, doc_ad_entity_ids, feature_vector)\n",
    "        set_feature_vector_cat_values(DOC_EVENT_CATEGORY_ID_FV, doc_event_category_ids, feature_vector)\n",
    "        set_feature_vector_cat_values(DOC_EVENT_TOPIC_ID_FV, doc_event_topic_ids, feature_vector)\n",
    "        set_feature_vector_cat_values(DOC_EVENT_ENTITY_ID_FV, doc_event_entity_ids, feature_vector)\n",
    "        \n",
    "        #Creating dummy column as the last column because xgboost have a problem if the last column is undefined for all rows, \n",
    "        #saying that dimentions of data and feature_names do not match\n",
    "        #feature_vector[feature_vector_labels_dict[DUMMY_FEATURE_COLUMN]] = float(0)\n",
    "            \n",
    "        #Ensuring that all elements are floats for compatibility with UDF output (ArrayType(FloatType()))\n",
    "        #feature_vector = list([float(x) for x in feature_vector])\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise Exception(\"[get_ad_feature_vector] ERROR PROCESSING FEATURE VECTOR! Params: {}\" \\\n",
    "                        .format([user_doc_ids_viewed, user_views_count, user_categories, user_topics, user_entities, \n",
    "                                 event_country, event_country_state,\n",
    "                            ad_id, document_id, source_id, doc_ad_publish_time, timestamp_event, platform_event,\n",
    "                            geo_location_event, \n",
    "                            doc_event_source_id, doc_event_publisher_id, doc_event_publish_time,\n",
    "                            traffic_source_pv, advertiser_id, publisher_id,\n",
    "                            campaign_id, document_id_event,\n",
    "                            doc_ad_category_ids, doc_ad_cat_confidence_levels, \n",
    "                            doc_ad_topic_ids, doc_ad_top_confidence_levels,\n",
    "                            doc_ad_entity_ids, doc_ad_ent_confidence_levels,\n",
    "                            doc_event_category_ids, doc_event_cat_confidence_levels,\n",
    "                            doc_event_topic_ids, doc_event_top_confidence_levels,\n",
    "                            doc_event_entity_ids, doc_event_ent_confidence_levels]),\n",
    "                        e)\n",
    "    \n",
    "    return SparseVector(len(feature_vector_labels_dict), feature_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "get_ad_feature_vector_udf = F.udf(lambda user_doc_ids_viewed, user_views_count, user_categories, user_topics, \n",
    "                                        user_entities, event_country, event_country_state, ad_id, document_id, source_id, \n",
    "                                        doc_ad_publish_time, timestamp_event, platform_event,\n",
    "                                        geo_location_event, \n",
    "                                        doc_event_source_id, doc_event_publisher_id, doc_event_publish_time,\n",
    "                                        traffic_source_pv, advertiser_id, publisher_id,\n",
    "                                        campaign_id, document_id_event,\n",
    "                                        category_ids_by_doc, cat_confidence_level_by_doc, \n",
    "                                        topic_ids_by_doc, top_confidence_level_by_doc,\n",
    "                                        entity_ids_by_doc, ent_confidence_level_by_doc,\n",
    "                                        doc_event_category_id_list, doc_event_confidence_level_cat_list,\n",
    "                                        doc_event_topic_id_list, doc_event_confidence_level_top,\n",
    "                                        doc_event_entity_id_list, doc_event_confidence_level_ent: \\\n",
    "                                         get_ad_feature_vector(user_doc_ids_viewed, user_views_count, user_categories, user_topics, user_entities, \n",
    "                                                            event_country, event_country_state, \n",
    "                                                            ad_id, document_id, source_id, doc_ad_publish_time, timestamp_event, platform_event,\n",
    "                                                            geo_location_event, \n",
    "                                                            doc_event_source_id, doc_event_publisher_id, doc_event_publish_time,   \n",
    "                                                            traffic_source_pv, advertiser_id, publisher_id,\n",
    "                                                            campaign_id, document_id_event,\n",
    "                                                            category_ids_by_doc, cat_confidence_level_by_doc, \n",
    "                                                            topic_ids_by_doc, top_confidence_level_by_doc,\n",
    "                                                            entity_ids_by_doc, ent_confidence_level_by_doc,\n",
    "                                                            doc_event_category_id_list, doc_event_confidence_level_cat_list,\n",
    "                                                            doc_event_topic_id_list, doc_event_confidence_level_top,\n",
    "                                                            doc_event_entity_id_list, doc_event_confidence_level_ent),    \n",
    "                            VectorUDT())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Building feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def set_feature_vector_cat_value_integral(field_name, field_value, feature_vector):\n",
    "    if not is_null(field_value): #and str(field_value) != '-1':\n",
    "        feature_vector[feature_vector_labels_integral_dict[field_name]] = float(field_value)\n",
    "        \n",
    "def set_feature_vector_cat_top_multi_values_integral(field_name, values, confidences, feature_vector, top=5):\n",
    "    top_values = list(filter(lambda z: z != -1, map(lambda y: y[0], sorted(zip(values, confidences), key=lambda x: -x[1]))))[:top]\n",
    "    for idx, field_value in list(enumerate(top_values)):\n",
    "        set_feature_vector_cat_value_integral('{}_{}'.format(field_name, idx+1), field_value, feature_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_ad_feature_vector_integral(user_doc_ids_viewed, user_views_count, user_categories, user_topics, user_entities, \n",
    "                            event_country, event_country_state,\n",
    "                            ad_id, document_id, source_id, doc_ad_publish_time, timestamp_event, platform_event,\n",
    "                            geo_location_event, \n",
    "                            doc_event_source_id, doc_event_publisher_id, doc_event_publish_time,\n",
    "                            traffic_source_pv, advertiser_id, publisher_id,\n",
    "                            campaign_id, document_id_event,\n",
    "                            doc_ad_category_ids, doc_ad_cat_confidence_levels, \n",
    "                            doc_ad_topic_ids, doc_ad_top_confidence_levels,\n",
    "                            doc_ad_entity_ids, doc_ad_ent_confidence_levels,\n",
    "                            doc_event_category_ids, doc_event_cat_confidence_levels,\n",
    "                            doc_event_topic_ids, doc_event_top_confidence_levels,\n",
    "                            doc_event_entity_ids, doc_event_ent_confidence_levels):\n",
    "             \n",
    "    try:\n",
    "\n",
    "        feature_vector = {}\n",
    "        \n",
    "        if user_views_count != None:\n",
    "            feature_vector[feature_vector_labels_integral_dict['user_views']] = float(user_views_count)\n",
    "         \n",
    "        if user_doc_ids_viewed != None:\n",
    "            feature_vector[feature_vector_labels_integral_dict['user_has_already_viewed_doc']] = float(document_id in user_doc_ids_viewed)               \n",
    "          \n",
    "        if ad_id in ad_id_popularity_broad.value:            \n",
    "            feature_vector[feature_vector_labels_integral_dict['ad_views']] = float(ad_id_popularity_broad.value[ad_id][1])\n",
    "        \n",
    "        if document_id in document_id_popularity_broad.value:\n",
    "            feature_vector[feature_vector_labels_integral_dict['doc_views']] = float(document_id_popularity_broad.value[document_id][1])            \n",
    "            \n",
    "        if timestamp_event > -1:\n",
    "            dt_timestamp_event = convert_odd_timestamp(timestamp_event)\n",
    "            if doc_ad_publish_time != None:\n",
    "                delta_days = (dt_timestamp_event - doc_ad_publish_time).days\n",
    "                if delta_days >= 0 and delta_days <= 365*10: #10 years\n",
    "                    feature_vector[feature_vector_labels_integral_dict['doc_ad_days_since_published']] = float(delta_days)\n",
    "                        \n",
    "            if doc_event_publish_time != None:\n",
    "                delta_days = (dt_timestamp_event - doc_event_publish_time).days\n",
    "                if delta_days >= 0 and delta_days <= 365*10: #10 years\n",
    "                    feature_vector[feature_vector_labels_integral_dict['doc_event_days_since_published']] = float(delta_days)\n",
    "                    \n",
    "            \n",
    "            #Local period of the day (hours)\n",
    "            dt_local_timestamp_event = get_local_datetime(dt_timestamp_event, event_country, event_country_state)    \n",
    "            local_hour_bin = get_hour_bin(dt_local_timestamp_event.hour)            \n",
    "            feature_vector[feature_vector_labels_integral_dict['doc_event_hour']] = float(local_hour_bin) #Hour for Decision Trees\n",
    "            set_feature_vector_cat_value_integral(EVENT_HOUR_FV, local_hour_bin, feature_vector) #Period of day for FFM\n",
    "            \n",
    "            #Weekend\n",
    "            weekend = int(is_weekend(dt_local_timestamp_event))\n",
    "            feature_vector[feature_vector_labels_integral_dict['event_weekend']] = float(weekend)               \n",
    "                                        \n",
    "        \n",
    "        conf_field_suffix = '_conf'\n",
    "        conf_multiplied_field_suffix = '_conf_multipl'\n",
    "        \n",
    "        #Setting Popularity fields\n",
    "        pop_scores = get_popularity_score(event_country, ad_id, document_id, source_id, \n",
    "                                publisher_id, advertiser_id, campaign_id, document_id_event,\n",
    "                                doc_ad_category_ids, doc_ad_cat_confidence_levels, \n",
    "                                doc_ad_topic_ids, doc_ad_top_confidence_levels,\n",
    "                                doc_ad_entity_ids, doc_ad_ent_confidence_levels,\n",
    "                                output_detailed_list=True)\n",
    "        \n",
    "                                \n",
    "\n",
    "        for score in pop_scores:\n",
    "            feature_vector[feature_vector_labels_integral_dict[score[0]]] = score[1]\n",
    "            feature_vector[feature_vector_labels_integral_dict[score[0]+conf_field_suffix]] = score[2]\n",
    "            feature_vector[feature_vector_labels_integral_dict[score[0]+conf_multiplied_field_suffix]] = score[1] * score[2]\n",
    "\n",
    "        #Setting User-Doc_ad CB Similarity fields\n",
    "        user_doc_ad_cb_sim_scores = get_user_cb_interest_score(user_views_count, user_categories, user_topics, user_entities, \n",
    "                                timestamp_event, \n",
    "                                 doc_ad_category_ids, doc_ad_cat_confidence_levels, \n",
    "                                 doc_ad_topic_ids, doc_ad_top_confidence_levels,\n",
    "                                 doc_ad_entity_ids, doc_ad_ent_confidence_levels,\n",
    "                                output_detailed_list=True)\n",
    "\n",
    "        for score in user_doc_ad_cb_sim_scores:\n",
    "            feature_vector[feature_vector_labels_integral_dict[score[0]]] = score[1]\n",
    "            feature_vector[feature_vector_labels_integral_dict[score[0]+conf_field_suffix]] = score[2]\n",
    "            feature_vector[feature_vector_labels_integral_dict[score[0]+conf_multiplied_field_suffix]] = score[1] * score[2]\n",
    "            \n",
    "        #Setting Doc_event-doc_ad CB Similarity fields\n",
    "        doc_event_doc_ad_cb_sim_scores = get_doc_event_doc_ad_cb_similarity_score(\n",
    "                                            doc_event_category_ids, doc_event_cat_confidence_levels,\n",
    "                                            doc_event_topic_ids, doc_event_top_confidence_levels,\n",
    "                                            doc_event_entity_ids, doc_event_ent_confidence_levels,\n",
    "                                            doc_ad_category_ids, doc_ad_cat_confidence_levels, \n",
    "                                            doc_ad_topic_ids, doc_ad_top_confidence_levels,\n",
    "                                            doc_ad_entity_ids, doc_ad_ent_confidence_levels,\n",
    "                                        output_detailed_list=True)\n",
    "        \n",
    "        for score in doc_event_doc_ad_cb_sim_scores:\n",
    "            feature_vector[feature_vector_labels_integral_dict[score[0]]] = score[1]\n",
    "            feature_vector[feature_vector_labels_integral_dict[score[0]+conf_field_suffix]] = score[2]\n",
    "            feature_vector[feature_vector_labels_integral_dict[score[0]+conf_multiplied_field_suffix]] = score[1] * score[2]\n",
    "        \n",
    "        #Process code for event_country\n",
    "        if event_country in event_country_values_counts:\n",
    "            event_country_code = event_country_values_counts[event_country]\n",
    "        else:\n",
    "            event_country_code = event_country_values_counts[LESS_SPECIAL_CAT_VALUE]                        \n",
    "        set_feature_vector_cat_value_integral(EVENT_COUNTRY_FV, event_country_code, feature_vector)\n",
    "        \n",
    "        #Process code for event_country_state\n",
    "        if event_country_state in event_country_state_values_counts:\n",
    "            event_country_state_code = event_country_state_values_counts[event_country_state]\n",
    "        else:\n",
    "            event_country_state_code = event_country_state_values_counts[LESS_SPECIAL_CAT_VALUE]         \n",
    "        set_feature_vector_cat_value_integral(EVENT_COUNTRY_STATE_FV, event_country_state_code, feature_vector)\n",
    "                \n",
    "        #Process code for geo_location_event\n",
    "        if geo_location_event in event_geo_location_values_counts:\n",
    "            geo_location_event_code = event_geo_location_values_counts[geo_location_event]\n",
    "        else:\n",
    "            geo_location_event_code = event_geo_location_values_counts[LESS_SPECIAL_CAT_VALUE]\n",
    "        set_feature_vector_cat_value_integral(EVENT_GEO_LOCATION_FV, geo_location_event_code, feature_vector)   \n",
    "         \n",
    "        set_feature_vector_cat_value_integral(TRAFFIC_SOURCE_FV, traffic_source_pv, feature_vector)        \n",
    "        set_feature_vector_cat_value_integral(EVENT_PLATFORM_FV, platform_event, feature_vector)\n",
    "        set_feature_vector_cat_value_integral(AD_ADVERTISER_FV, advertiser_id, feature_vector)\n",
    "        set_feature_vector_cat_value_integral(DOC_AD_SOURCE_ID_FV, source_id, feature_vector)\n",
    "        set_feature_vector_cat_value_integral(DOC_AD_PUBLISHER_ID_FV, publisher_id, feature_vector)\n",
    "        set_feature_vector_cat_value_integral(DOC_EVENT_SOURCE_ID_FV, doc_event_source_id, feature_vector)\n",
    "        set_feature_vector_cat_value_integral(DOC_EVENT_PUBLISHER_ID_FV, doc_event_publisher_id, feature_vector)\n",
    "                \n",
    "        set_feature_vector_cat_top_multi_values_integral(DOC_AD_CATEGORY_ID_FV, doc_ad_category_ids, doc_ad_cat_confidence_levels, feature_vector, top=3)\n",
    "        set_feature_vector_cat_top_multi_values_integral(DOC_AD_TOPIC_ID_FV, doc_ad_topic_ids, doc_ad_top_confidence_levels, feature_vector, top=3)\n",
    "        \n",
    "        set_feature_vector_cat_top_multi_values_integral(DOC_EVENT_CATEGORY_ID_FV, doc_event_category_ids, doc_event_cat_confidence_levels, feature_vector, top=3)\n",
    "        set_feature_vector_cat_top_multi_values_integral(DOC_EVENT_TOPIC_ID_FV, doc_event_topic_ids, doc_event_top_confidence_levels, feature_vector, top=3)                           \n",
    "        \n",
    "        #Process codes for doc_ad_entity_ids\n",
    "        doc_ad_entity_ids_codes = [doc_entity_id_values_counts[x] if x in doc_entity_id_values_counts \n",
    "                                   else doc_entity_id_values_counts[LESS_SPECIAL_CAT_VALUE] \n",
    "                                   for x in doc_ad_entity_ids]\n",
    "        set_feature_vector_cat_top_multi_values_integral(DOC_AD_ENTITY_ID_FV, doc_ad_entity_ids_codes, doc_ad_ent_confidence_levels, feature_vector, top=6)\n",
    "        \n",
    "        \n",
    "        #Process codes for doc_event_entity_ids\n",
    "        doc_event_entity_ids_codes = [doc_entity_id_values_counts[x] if x in doc_entity_id_values_counts \n",
    "                                   else doc_entity_id_values_counts[LESS_SPECIAL_CAT_VALUE] \n",
    "                                   for x in doc_event_entity_ids]\n",
    "        set_feature_vector_cat_top_multi_values_integral(DOC_EVENT_ENTITY_ID_FV, doc_event_entity_ids_codes, doc_event_ent_confidence_levels, feature_vector, top=6)\n",
    "        \n",
    "        #Creating dummy column as the last column because xgboost have a problem if the last column is undefined for all rows, \n",
    "        #saying that dimentions of data and feature_names do not match\n",
    "        #feature_vector[feature_vector_labels_dict[DUMMY_FEATURE_COLUMN]] = float(0)\n",
    "            \n",
    "        #Ensuring that all elements are floats for compatibility with UDF output (ArrayType(FloatType()))\n",
    "        #feature_vector = list([float(x) for x in feature_vector])\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise Exception(\"[get_ad_feature_vector_integral] ERROR PROCESSING FEATURE VECTOR! Params: {}\" \\\n",
    "                        .format([user_doc_ids_viewed, user_views_count, user_categories, user_topics, user_entities, \n",
    "                                 event_country, event_country_state,\n",
    "                            ad_id, document_id, source_id, doc_ad_publish_time, timestamp_event, platform_event,\n",
    "                            geo_location_event, \n",
    "                            doc_event_source_id, doc_event_publisher_id, doc_event_publish_time,\n",
    "                            traffic_source_pv, advertiser_id, publisher_id,\n",
    "                            campaign_id, document_id_event,\n",
    "                            doc_ad_category_ids, doc_ad_cat_confidence_levels, \n",
    "                            doc_ad_topic_ids, doc_ad_top_confidence_levels,\n",
    "                            doc_ad_entity_ids, doc_ad_ent_confidence_levels,\n",
    "                            doc_event_category_ids, doc_event_cat_confidence_levels,\n",
    "                            doc_event_topic_ids, doc_event_top_confidence_levels,\n",
    "                            doc_event_entity_ids, doc_event_ent_confidence_levels]),\n",
    "                        e)\n",
    "    \n",
    "    return SparseVector(len(feature_vector_labels_integral_dict), feature_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "get_ad_feature_vector_integral_udf = F.udf(lambda user_doc_ids_viewed, user_views_count, user_categories, user_topics, \n",
    "                                        user_entities, event_country, event_country_state, ad_id, document_id, source_id, \n",
    "                                        doc_ad_publish_time, timestamp_event, platform_event,\n",
    "                                        geo_location_event, \n",
    "                                        doc_event_source_id, doc_event_publisher_id, doc_event_publish_time,\n",
    "                                        traffic_source_pv, advertiser_id, publisher_id,\n",
    "                                        campaign_id, document_id_event,\n",
    "                                        category_ids_by_doc, cat_confidence_level_by_doc, \n",
    "                                        topic_ids_by_doc, top_confidence_level_by_doc,\n",
    "                                        entity_ids_by_doc, ent_confidence_level_by_doc,\n",
    "                                        doc_event_category_id_list, doc_event_confidence_level_cat_list,\n",
    "                                        doc_event_topic_id_list, doc_event_confidence_level_top,\n",
    "                                        doc_event_entity_id_list, doc_event_confidence_level_ent: \\\n",
    "                                         get_ad_feature_vector_integral(user_doc_ids_viewed, user_views_count, user_categories, user_topics, user_entities, \n",
    "                                                            event_country, event_country_state, \n",
    "                                                            ad_id, document_id, source_id, doc_ad_publish_time, timestamp_event, platform_event,\n",
    "                                                            geo_location_event, \n",
    "                                                            doc_event_source_id, doc_event_publisher_id, doc_event_publish_time,   \n",
    "                                                            traffic_source_pv, advertiser_id, publisher_id,\n",
    "                                                            campaign_id, document_id_event,\n",
    "                                                            category_ids_by_doc, cat_confidence_level_by_doc, \n",
    "                                                            topic_ids_by_doc, top_confidence_level_by_doc,\n",
    "                                                            entity_ids_by_doc, ent_confidence_level_by_doc,\n",
    "                                                            doc_event_category_id_list, doc_event_confidence_level_cat_list,\n",
    "                                                            doc_event_topic_id_list, doc_event_confidence_level_top,\n",
    "                                                            doc_event_entity_id_list, doc_event_confidence_level_ent),    \n",
    "                            VectorUDT())\n",
    "                             #StructField(\"features\", VectorUDT()))\n",
    "                             #MapType(IntegerType(), FloatType())) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Export Train set feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_set_enriched_df = train_set_df \\                            \n",
    "                             .join(documents_categories_grouped_df, on=F.col(\"document_id_promo\") == F.col(\"documents_categories_grouped.document_id_cat\"), how='left') \\\n",
    "                             .join(documents_topics_grouped_df, on=F.col(\"document_id_promo\") == F.col(\"documents_topics_grouped.document_id_top\"), how='left') \\\n",
    "                             .join(documents_entities_grouped_df, on=F.col(\"document_id_promo\") == F.col(\"documents_entities_grouped.document_id_ent\"), how='left') \\\n",
    "                             .join(documents_categories_grouped_df \\\n",
    "                                       .withColumnRenamed('category_id_list', 'doc_event_category_id_list')\n",
    "                                       .withColumnRenamed('confidence_level_cat_list', 'doc_event_confidence_level_cat_list') \\\n",
    "                                       .alias('documents_event_categories_grouped'), \n",
    "                                   on=F.col(\"document_id_event\") == F.col(\"documents_event_categories_grouped.document_id_cat\"), \n",
    "                                   how='left') \\\n",
    "                             .join(documents_topics_grouped_df \\\n",
    "                                       .withColumnRenamed('topic_id_list', 'doc_event_topic_id_list')\n",
    "                                       .withColumnRenamed('confidence_level_top_list', 'doc_event_confidence_level_top_list') \\\n",
    "                                       .alias('documents_event_topics_grouped'), \n",
    "                                   on=F.col(\"document_id_event\") == F.col(\"documents_event_topics_grouped.document_id_top\"), \n",
    "                                   how='left') \\\n",
    "                             .join(documents_entities_grouped_df \\\n",
    "                                       .withColumnRenamed('entity_id_list', 'doc_event_entity_id_list')\n",
    "                                       .withColumnRenamed('confidence_level_ent_list', 'doc_event_confidence_level_ent_list') \\\n",
    "                                       .alias('documents_event_entities_grouped'), \n",
    "                                   on=F.col(\"document_id_event\") == F.col(\"documents_event_entities_grouped.document_id_ent\"), \n",
    "                                   how='left') \\\n",
    "                            .select('display_id','uuid_event','event_country','event_country_state','platform_event',\n",
    "                                    'source_id_doc_event', 'publisher_doc_event','publish_time_doc_event',\n",
    "                                            'publish_time', 'ad_id','document_id_promo','clicked',   \n",
    "                                           'geo_location_event', 'advertiser_id', 'publisher_id',\n",
    "                                            'campaign_id', 'document_id_event',\n",
    "                                            'traffic_source_pv',                                          \n",
    "                                        int_list_null_to_empty_list_udf('doc_event_category_id_list').alias('doc_event_category_id_list'),\n",
    "                                        float_list_null_to_empty_list_udf('doc_event_confidence_level_cat_list').alias('doc_event_confidence_level_cat_list'),\n",
    "                                        int_list_null_to_empty_list_udf('doc_event_topic_id_list').alias('doc_event_topic_id_list'),\n",
    "                                        float_list_null_to_empty_list_udf('doc_event_confidence_level_top_list').alias('doc_event_confidence_level_top_list'),\n",
    "                                        str_list_null_to_empty_list_udf('doc_event_entity_id_list').alias('doc_event_entity_id_list'),\n",
    "                                        float_list_null_to_empty_list_udf('doc_event_confidence_level_ent_list').alias('doc_event_confidence_level_ent_list'),\n",
    "                                       int_null_to_minus_one_udf('source_id').alias('source_id'),                                      \n",
    "                                       int_null_to_minus_one_udf('timestamp_event').alias('timestamp_event'),\n",
    "                                       int_list_null_to_empty_list_udf('category_id_list').alias('category_id_list'), \n",
    "                                       float_list_null_to_empty_list_udf('confidence_level_cat_list').alias('confidence_level_cat_list'), \n",
    "                                       int_list_null_to_empty_list_udf('topic_id_list').alias('topic_id_list'), \n",
    "                                       float_list_null_to_empty_list_udf('confidence_level_top_list').alias('confidence_level_top_list'), \n",
    "                                       str_list_null_to_empty_list_udf('entity_id_list').alias('entity_id_list'), \n",
    "                                       float_list_null_to_empty_list_udf('confidence_level_ent_list').alias('confidence_level_ent_list')                                                       \n",
    "                                      ) \\\n",
    "                            .join(user_profiles_df, on=[F.col(\"user_profiles.uuid\") == F.col(\"uuid_event\")], how='left') \\\n",
    "                            .withColumnRenamed('categories', 'user_categories') \\\n",
    "                            .withColumnRenamed('topics', 'user_topics') \\\n",
    "                            .withColumnRenamed('entities', 'user_entities') \\\n",
    "                            .withColumnRenamed('doc_ids', 'user_doc_ids_viewed') \\\n",
    "                            .withColumnRenamed('views', 'user_views_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_set_feature_vectors_df = train_set_enriched_df \\\n",
    "                                .withColumn('feature_vector', \n",
    "                                            #get_ad_feature_vector_udf(\n",
    "                                            get_ad_feature_vector_integral_udf(\n",
    "                                                                'user_doc_ids_viewed',\n",
    "                                                                'user_views_count',\n",
    "                                                                'user_categories', \n",
    "                                                                'user_topics', \n",
    "                                                                'user_entities', \n",
    "                                                                'event_country', \n",
    "                                                                'event_country_state',\n",
    "                                                                'ad_id', \n",
    "                                                                'document_id_promo', \n",
    "                                                                'source_id', \n",
    "                                                                'publish_time', \n",
    "                                                                'timestamp_event', \n",
    "                                                                'platform_event',\n",
    "                                                                'geo_location_event', \n",
    "                                                                'source_id_doc_event', \n",
    "                                                                'publisher_doc_event',\n",
    "                                                                'publish_time_doc_event',\n",
    "                                                                'traffic_source_pv',\n",
    "                                                                'advertiser_id', \n",
    "                                                                'publisher_id',\n",
    "                                                                'campaign_id',\n",
    "                                                                'document_id_event',\n",
    "                                                                'category_id_list', \n",
    "                                                                'confidence_level_cat_list', \n",
    "                                                                'topic_id_list', \n",
    "                                                                'confidence_level_top_list',\n",
    "                                                                'entity_id_list', \n",
    "                                                                'confidence_level_ent_list',\n",
    "                                                                'doc_event_category_id_list',\n",
    "                                                                'doc_event_confidence_level_cat_list',\n",
    "                                                                'doc_event_topic_id_list',\n",
    "                                                                'doc_event_confidence_level_top_list',\n",
    "                                                                'doc_event_entity_id_list',\n",
    "                                                                'doc_event_confidence_level_ent_list')) \\\n",
    "                            .select(F.col('uuid_event').alias('uuid'),\n",
    "                                    'display_id',\n",
    "                                    'ad_id',\n",
    "                                    'document_id_event',\n",
    "                                    F.col('document_id_promo').alias('document_id'),\n",
    "                                    F.col('clicked').alias('label'),\n",
    "                                    'feature_vector') #\\\n",
    "                            #.orderBy('display_id','ad_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if evaluation:\n",
    "    train_feature_vector_gcs_folder_name = 'train_feature_vectors_integral_eval'\n",
    "else:\n",
    "    train_feature_vector_gcs_folder_name = 'train_feature_vectors_integral'    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 732 ms, sys: 144 ms, total: 876 ms\n",
      "Wall time: 1h 20min 54s\n"
     ]
    }
   ],
   "source": [
    "%time train_set_feature_vectors_df.write.parquet(OUTPUT_BUCKET_FOLDER+train_feature_vector_gcs_folder_name, mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Exporting integral feature vectors to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if evaluation:\n",
    "    train_feature_vector_integral_csv_folder_name = 'train_feature_vectors_integral_eval.csv'\n",
    "else:\n",
    "    train_feature_vector_integral_csv_folder_name = 'train_feature_vectors_integral.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "integral_headers = ['label', 'display_id', 'ad_id', 'doc_id', 'doc_event_id', 'is_leak'] + feature_vector_labels_integral\n",
    "    \n",
    "with open(train_feature_vector_integral_csv_folder_name+\".header\", 'w') as output:\n",
    "    output.writelines('\\n'.join(integral_headers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_feature_vectors_integral_csv_rdd = train_feature_vectors_exported_df.select(\n",
    "     'label', 'display_id', 'ad_id', 'document_id', 'document_id_event', 'feature_vector').withColumn('is_leak', F.lit(-1)) \\\n",
    "     .rdd.map(lambda x: sparse_vector_to_csv_with_nulls_row([x['label'], x['display_id'], x['ad_id'], x['document_id'], x['document_id_event'], x['is_leak']], \n",
    "                                                  x['feature_vector'], len(integral_headers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%time train_feature_vectors_integral_csv_rdd.saveAsTextFile(OUTPUT_BUCKET_FOLDER+train_feature_vector_integral_csv_folder_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Export Validation/Test set feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def is_leak(max_timestamp_pv_leak, timestamp_event):\n",
    "    return max_timestamp_pv_leak >= 0 and max_timestamp_pv_leak >= timestamp_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "is_leak_udf = F.udf(lambda max_timestamp_pv_leak, timestamp_event: int(is_leak(max_timestamp_pv_leak, timestamp_event)), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if evaluation:\n",
    "    data_df = validation_set_df\n",
    "else:\n",
    "    data_df = test_set_df\n",
    "\n",
    "\n",
    "test_validation_set_enriched_df = data_df.select('display_id','uuid_event','event_country','event_country_state','platform_event',\n",
    "                                            'source_id_doc_event', 'publisher_doc_event','publish_time_doc_event',     \n",
    "                                            'publish_time',\n",
    "                                           'ad_id','document_id_promo','clicked',  \n",
    "                                           'geo_location_event', 'advertiser_id', 'publisher_id',\n",
    "                                           'campaign_id', 'document_id_event',\n",
    "                                           'traffic_source_pv',                                           \n",
    "                                        int_list_null_to_empty_list_udf('doc_event_category_id_list').alias('doc_event_category_id_list'),\n",
    "                                        float_list_null_to_empty_list_udf('doc_event_confidence_level_cat_list').alias('doc_event_confidence_level_cat_list'),\n",
    "                                        int_list_null_to_empty_list_udf('doc_event_topic_id_list').alias('doc_event_topic_id_list'),\n",
    "                                        float_list_null_to_empty_list_udf('doc_event_confidence_level_top_list').alias('doc_event_confidence_level_top_list'),\n",
    "                                        str_list_null_to_empty_list_udf('doc_event_entity_id_list').alias('doc_event_entity_id_list'),\n",
    "                                        float_list_null_to_empty_list_udf('doc_event_confidence_level_ent_list').alias('doc_event_confidence_level_ent_list'),\n",
    "                                       int_null_to_minus_one_udf('source_id').alias('source_id'),                                   \n",
    "                                       int_null_to_minus_one_udf('timestamp_event').alias('timestamp_event'),\n",
    "                                       int_list_null_to_empty_list_udf('category_id_list').alias('category_id_list'), \n",
    "                                       float_list_null_to_empty_list_udf('confidence_level_cat_list').alias('confidence_level_cat_list'), \n",
    "                                       int_list_null_to_empty_list_udf('topic_id_list').alias('topic_id_list'), \n",
    "                                       float_list_null_to_empty_list_udf('confidence_level_top_list').alias('confidence_level_top_list'), \n",
    "                                       str_list_null_to_empty_list_udf('entity_id_list').alias('entity_id_list'), \n",
    "                                       float_list_null_to_empty_list_udf('confidence_level_ent_list').alias('confidence_level_ent_list'),\n",
    "                                       int_null_to_minus_one_udf('max_timestamp_pv').alias('max_timestamp_pv_leak')\n",
    "                                      ) \\\n",
    "                            .join(user_profiles_df, on=[F.col(\"user_profiles.uuid\") == F.col(\"uuid_event\")], how='left') \\\n",
    "                            .withColumnRenamed('categories', 'user_categories') \\\n",
    "                            .withColumnRenamed('topics', 'user_topics') \\\n",
    "                            .withColumnRenamed('entities', 'user_entities') \\\n",
    "                            .withColumnRenamed('doc_ids', 'user_doc_ids_viewed') \\\n",
    "                            .withColumnRenamed('views', 'user_views_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_validation_set_feature_vectors_df = test_validation_set_enriched_df \\\n",
    "                                .withColumn('feature_vector', \n",
    "                                            #get_ad_feature_vector_udf(\n",
    "                                            get_ad_feature_vector_integral_udf(\n",
    "                                                                'user_doc_ids_viewed', \n",
    "                                                                'user_views_count',\n",
    "                                                                'user_categories', \n",
    "                                                                'user_topics', \n",
    "                                                                'user_entities', \n",
    "                                                                'event_country', \n",
    "                                                                'event_country_state',\n",
    "                                                                'ad_id', \n",
    "                                                                'document_id_promo', \n",
    "                                                                'source_id', \n",
    "                                                                'publish_time', \n",
    "                                                                'timestamp_event', \n",
    "                                                                'platform_event',\n",
    "                                                                'geo_location_event', \n",
    "                                                                'source_id_doc_event', \n",
    "                                                                'publisher_doc_event',\n",
    "                                                                'publish_time_doc_event',\n",
    "                                                                'traffic_source_pv',\n",
    "                                                                'advertiser_id', \n",
    "                                                                'publisher_id',\n",
    "                                                                'campaign_id',\n",
    "                                                                'document_id_event',\n",
    "                                                                'category_id_list', \n",
    "                                                                'confidence_level_cat_list', \n",
    "                                                                'topic_id_list', \n",
    "                                                                'confidence_level_top_list',\n",
    "                                                                'entity_id_list', \n",
    "                                                                'confidence_level_ent_list',\n",
    "                                                                'doc_event_category_id_list',\n",
    "                                                                'doc_event_confidence_level_cat_list',\n",
    "                                                                'doc_event_topic_id_list',\n",
    "                                                                'doc_event_confidence_level_top_list',\n",
    "                                                                'doc_event_entity_id_list',\n",
    "                                                                'doc_event_confidence_level_ent_list')) \\\n",
    "                            .select(F.col('uuid').alias('uuid'),                                    \n",
    "                                    'display_id',\n",
    "                                    'ad_id',\n",
    "                                    'document_id_event',\n",
    "                                    F.col('document_id_promo').alias('document_id'),\n",
    "                                    F.col('clicked').alias('label'),\n",
    "                                    is_leak_udf('max_timestamp_pv_leak','timestamp_event').alias('is_leak'),\n",
    "                                    'feature_vector') #\\\n",
    "                            #.orderBy('display_id','ad_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if evaluation:\n",
    "    test_validation_feature_vector_gcs_folder_name = 'validation_feature_vectors_integral'\n",
    "else:\n",
    "    test_validation_feature_vector_gcs_folder_name = 'test_feature_vectors_integral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%time test_validation_set_feature_vectors_df.write.parquet(OUTPUT_BUCKET_FOLDER+test_validation_feature_vector_gcs_folder_name, mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Exporting integral feature vectors to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if evaluation:\n",
    "    test_validation_feature_vector_integral_csv_folder_name = 'validation_feature_vectors_integral.csv'\n",
    "else:\n",
    "    test_validation_feature_vector_integral_csv_folder_name = 'test_feature_vectors_integral.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "integral_headers = ['label', 'display_id', 'ad_id', 'doc_id', 'doc_event_id', 'is_leak'] + feature_vector_labels_integral\n",
    "    \n",
    "with open(test_validation_feature_vector_integral_csv_folder_name+\".header\", 'w') as output:\n",
    "    output.writelines('\\n'.join(integral_headers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_validation_feature_vectors_integral_csv_rdd = test_validation_feature_vectors_exported_df.select(\n",
    "     'label', 'display_id', 'ad_id', 'document_id', 'document_id_event', 'is_leak', 'feature_vector') \\\n",
    "     .rdd.map(lambda x: sparse_vector_to_csv_with_nulls_row([x['label'], x['display_id'], x['ad_id'], x['document_id'], x['document_id_event'], x['is_leak']], \n",
    "                                                  x['feature_vector'], len(integral_headers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%time test_validation_feature_vectors_integral_csv_rdd.saveAsTextFile(OUTPUT_BUCKET_FOLDER+test_validation_feature_vector_integral_csv_folder_name)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
